"""
Feedbackå®Ÿé¨“ç”¨SAEåˆ†æå™¨

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€feedback.jsonlãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ã€LLMã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«å¯¾ã™ã‚‹
å¿œç­”ã¨ãã®éš›ã®SAEå†…éƒ¨çŠ¶æ…‹ã‚’åˆ†æã—ã¾ã™ã€‚
"""

import json
import os
import gc
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
import torch
import numpy as np
from datetime import datetime
from tqdm import tqdm

# ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãŸã‚ã®ç’°å¢ƒå¤‰æ•°è¨­å®š
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
warnings.filterwarnings('ignore')

# SAE Lens imports
from transformer_lens import HookedTransformer
from sae_lens import SAE


@dataclass
class FeedbackPromptInfo:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±"""
    dataset: str
    prompt_template_type: str
    prompt: str
    base_data: Dict[str, Any]  # å…ƒã®baseãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ


@dataclass
class FeedbackResponse:
    """1ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹å¿œç­”ã¨SAEçŠ¶æ…‹"""
    prompt_info: FeedbackPromptInfo
    response_text: str
    sae_activations: Dict[str, Any]  # {feature_id: activation_value}
    top_k_features: List[Tuple[int, float]]  # [(feature_id, value), ...]
    metadata: Dict[str, Any]


@dataclass
class FeedbackQuestionResult:
    """1ã¤ã®è³ªå•ï¼ˆ5ã¤ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã®åˆ†æçµæœ"""
    question_id: int
    dataset: str
    base_text: str
    variations: List[FeedbackResponse]
    timestamp: str


class FeedbackAnalyzer:
    """Feedbackå®Ÿé¨“ç”¨ã®SAEåˆ†æå™¨"""
    
    def __init__(self, config):
        """
        åˆæœŸåŒ–
        
        Args:
            config: ExperimentConfig ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        self.config = config
        self.model = None
        self.sae = None
        self.results: List[FeedbackQuestionResult] = []
        
        # Feedbackå°‚ç”¨è¨­å®šã®å–å¾—
        self.feedback_config = getattr(config, 'feedback', None)
        if self.feedback_config is None:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
            from config import FeedbackConfig
            self.feedback_config = FeedbackConfig()
        
        # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        self.results_dir = Path("results/feedback")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # å‡¦ç†ç¯„å›²ã‚’è¨˜éŒ²ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åãƒ»ãƒ­ã‚°ç”¨ï¼‰
        self.processed_start_id = None
        self.processed_end_id = None
        
        if self.config.debug.verbose:
            print("ğŸ”§ FeedbackAnalyzer initialized")
            print(f"   ğŸ“ Results directory: {self.results_dir}")
            print(f"   ğŸ’¾ Prompt tokens: {'å…¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒˆãƒ¼ã‚¯ãƒ³' if self.feedback_config.save_all_tokens else 'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€çµ‚ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ï¼ˆæ¨å¥¨ï¼‰'}")
            print(f"   ğŸ’¬ Response tokens: æœ€åˆã®{self.feedback_config.response_tokens_to_capture}ãƒˆãƒ¼ã‚¯ãƒ³{'ï¼ˆå–å¾—ã™ã‚‹ï¼‰' if self.feedback_config.response_tokens_to_capture > 0 else 'ï¼ˆå–å¾—ã—ãªã„ï¼‰'}")
            print(f"   ğŸ¯ Target layer: {self.feedback_config.target_layer}")
            print(f"   ğŸ“ åˆ†æä½ç½®: A) å¿œç­”ç”Ÿæˆç›´å‰ï¼ˆæ„å›³ï¼‰+ B) å¿œç­”æœ€åˆã®æ•°ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå®Ÿè¡Œï¼‰")
            
    def get_model_device(self) -> str:
        """ãƒ¢ãƒ‡ãƒ«ã®ç¾åœ¨ã®ãƒ‡ãƒã‚¤ã‚¹ã‚’å®‰å…¨ã«å–å¾—"""
        if self.model is None:
            return self.device
        try:
            first_param = next(self.model.parameters())
            return str(first_param.device)
        except (StopIteration, AttributeError):
            return self.device

    def get_current_sae_device(self) -> str:
        """SAEã®ç¾åœ¨ã®ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—"""
        if self.sae is None:
            return self.device
        try:
            first_param = next(self.sae.parameters())
            return str(first_param.device)
        except (StopIteration, AttributeError):
            return self.sae_device if self.sae_device else self.device

    def ensure_device_consistency(self, tensor: torch.Tensor) -> torch.Tensor:
        """ãƒ†ãƒ³ã‚½ãƒ«ã‚’SAEã¨åŒã˜ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•"""
        if self.sae is None:
            return tensor
        sae_device = self.get_current_sae_device()
        if str(tensor.device) != sae_device:
            tensor = tensor.to(sae_device)
        return tensor

    def optimize_memory_usage(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€é©åŒ–"""
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                if self.config.debug.verbose:
                    memory_allocated = torch.cuda.memory_allocated() / 1e9
                    memory_reserved = torch.cuda.memory_reserved() / 1e9
                    print(f"ğŸ’¾ GPU Memory: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved")
            gc.collect()
        except Exception as e:
            if self.config.debug.verbose:
                print(f"âš ï¸ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ä¸­ã«è­¦å‘Š: {e}")

    def force_clear_gpu_cache(self):
        """GPUã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å¼·åˆ¶çš„ã«ã‚¯ãƒªã‚¢"""
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            gc.collect()
            if self.config.debug.verbose:
                print("ğŸ§¹ GPU cache cleared")
        except Exception as e:
            if self.config.debug.verbose:
                print(f"âš ï¸ GPU cache clear warning: {e}")    
    
    def load_feedback_data(self, data_path: Optional[str] = None) -> List[Dict]:
        """
        feedback.jsonlãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯configã‹ã‚‰å–å¾—ï¼‰
        
        Returns:
            èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        if data_path is None:
            data_path = self.config.data.dataset_path
        
        if self.config.debug.verbose:
            print(f"ğŸ“‚ Loading feedback data from: {data_path}")
        
        with open(data_path, 'r', encoding='utf-8') as f:
            data = [json.loads(line) for line in f]
        
        if self.config.debug.verbose:
            print(f"âœ… Loaded {len(data)} entries")
        
        return data
    
    def create_prompt(self, data: Dict) -> FeedbackPromptInfo:
        """
        ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±ã‚’ä½œæˆ
        
        Args:
            data: feedback.jsonlã®1ã‚¨ãƒ³ãƒˆãƒª
        
        Returns:
            FeedbackPromptInfo ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        dataset = data["base"]["dataset"]
        metadata = data["metadata"]
        prompt_template = metadata["prompt_template"]
        prompt_template_type = metadata["prompt_template_type"]
        
        if dataset == "arguments" or dataset == "poems":
            text = data["base"]["text"]
            prompt = prompt_template.format(text=text)
        elif dataset == "math":
            question = data["base"]["question"]
            correct_solution = data["base"]["correct_solution"]
            prompt = prompt_template.format(
                question=question, 
                correct_solution=correct_solution
            )
        else:
            raise ValueError(f"Unknown dataset: {dataset}")
        
        return FeedbackPromptInfo(
            dataset=dataset,
            prompt_template_type=prompt_template_type,
            prompt=prompt,
            base_data=data["base"]
        )
    
    def aggregate_prompts(self, feedback_data: List[Dict]) -> List[List[FeedbackPromptInfo]]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’5ã¤ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        
        Args:
            feedback_data: feedback.jsonlã®å…¨ãƒ‡ãƒ¼ã‚¿
        
        Returns:
            [[variation1, variation2, ..., variation5], ...] ã®å½¢å¼
        """
        prompt_variations = []
        prompt_groups = []
        
        for i, data in enumerate(feedback_data, 1):
            prompt_info = self.create_prompt(data)
            prompt_variations.append(prompt_info)
            
            # 5ã¤ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            if i % 5 == 0:
                prompt_groups.append(prompt_variations)
                prompt_variations = []
        
        # æ®‹ã‚ŠãŒã‚ã‚‹å ´åˆï¼ˆãƒ‡ãƒ¼ã‚¿ãŒ5ã®å€æ•°ã§ãªã„å ´åˆï¼‰
        if prompt_variations:
            prompt_groups.append(prompt_variations)
        
        if self.config.debug.verbose:
            print(f"ğŸ“¦ Grouped into {len(prompt_groups)} question sets")
        
        return prompt_groups
    
    def load_model_and_sae(self):
        """ãƒ¢ãƒ‡ãƒ«ã¨SAEã‚’ãƒ­ãƒ¼ãƒ‰"""
        if self.config.debug.verbose:
            print("ğŸ”„ Loading model and SAE...")
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        device = self.config.model.device
        if device == "auto":
            device = "cuda" if torch.cuda.is_available() else "cpu"
        
        if self.config.debug.verbose:
            print(f"   ğŸ–¥ï¸  Using device: {device}")
        
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        if self.config.debug.verbose:
            print(f"   ğŸ“¥ Loading model: {self.config.model.name}")
        
        dtype = torch.bfloat16 if getattr(self.config.model, 'use_bfloat16', False) else torch.float16
        
        self.model = HookedTransformer.from_pretrained_no_processing(
            self.config.model.name,
            device=device,
            dtype=dtype
        )
        
        # SAEã®ãƒ­ãƒ¼ãƒ‰
        if self.config.debug.verbose:
            print(f"   ğŸ“¥ Loading SAE: {self.config.model.sae_release}/{self.config.model.sae_id}")
        
        self.sae, _, _ = SAE.from_pretrained(
            release=self.config.model.sae_release,
            sae_id=self.config.model.sae_id,
            device=device
        )
        
        # SAEã®ãƒ‡ãƒã‚¤ã‚¹ã‚’è¨˜éŒ²
        self.sae_device = str(device)
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å–å¾—
        self.tokenizer = self.model.tokenizer
        
        if self.config.debug.verbose:
            print("âœ… Model and SAE loaded successfully")
            print(f"   ğŸ¯ Model device: {self.get_model_device()}")
            print(f"   ğŸ¯ SAE device: {self.get_current_sae_device()}")
            if torch.cuda.is_available():
                memory_allocated = torch.cuda.memory_allocated() / 1e9
                print(f"   ğŸ’¾ GPU Memory: {memory_allocated:.2f} GB")
    
    def generate_with_sae(self, prompt: str) -> Tuple[str, Dict[str, Any]]:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦ç”Ÿæˆã‚’å®Ÿè¡Œã—ã€SAEæ´»æ€§åŒ–ã‚’å–å¾—
        
        åˆ†æä½ç½®ï¼ˆMLå­¦ç¿’ã¨ã‚¹ãƒ†ãƒƒãƒ—4ä»‹å…¥å®Ÿé¨“ã®ãŸã‚ï¼‰:
        A. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå¿œç­”ç”Ÿæˆç›´å‰ã®ã€Œæ„å›³ãƒ»è¨ˆç”»ã€çŠ¶æ…‹ï¼‰ - å¸¸ã«å–å¾—
        B. å¿œç­”ã®æœ€åˆã®æ•°ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆè¿åˆçš„å¿œç­”ã®ã€Œå®Ÿè¡Œãƒ»ç¶­æŒã€çŠ¶æ…‹ï¼‰ - ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        
        ãƒ‡ãƒ¼ã‚¿ä¿å­˜æ–¹é‡ï¼ˆã‚¹ãƒ†ãƒƒãƒ—3ã®MLå­¦ç¿’ç”¨ï¼‰:
        - SAEã®æ´»æ€§åŒ–å€¤ãŒ0ã‚ˆã‚Šå¤§ãã„å…¨ã¦ã®ç‰¹å¾´ã‚’ä¿å­˜ï¼ˆç–ãƒ™ã‚¯ãƒˆãƒ«å½¢å¼ï¼‰
        - é–¾å€¤ã«ã‚ˆã‚‹äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯è¡Œã‚ãªã„ï¼ˆXGBoostã®ç‰¹å¾´é¸æŠã«å§”ã­ã‚‹ï¼‰
        - ã“ã‚Œã«ã‚ˆã‚Šã€SHAPåˆ†æã§å…¨ç‰¹å¾´ã®å¯„ä¸åº¦ã‚’æ­£ç¢ºã«è©•ä¾¡å¯èƒ½
        
        ä¿å­˜å½¢å¼ä¾‹:
        {
            "prompt_last_token": {  # A: å¿œç­”ç”Ÿæˆç›´å‰ã®ã€Œæ„å›³ã€çŠ¶æ…‹
                "15": 0.523,
                "1024": 3.217,
                ...
            },
            "response_token_0": {  # B: å¿œç­”1ãƒˆãƒ¼ã‚¯ãƒ³ç›®ã®ã€Œå®Ÿè¡Œã€çŠ¶æ…‹
                "23": 0.412,
                "2048": 1.853,
                ...
            },
            "response_token_1": { ... },  # å¿œç­”2ãƒˆãƒ¼ã‚¯ãƒ³ç›®
            ...  # response_tokens_to_captureã®è¨­å®šå€¤ã¾ã§
        }
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        
        Returns:
            (ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ, SAEæ´»æ€§åŒ–æƒ…å ±)
        """
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokens = self.model.to_tokens(prompt)
        original_length = tokens.shape[1]  # å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨˜éŒ²
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãã§ç”Ÿæˆå®Ÿè¡Œ
        with torch.no_grad():
            # ç”Ÿæˆå®Ÿè¡Œ
            generated_tokens = self.model.generate(
                tokens,
                max_new_tokens=self.config.generation.max_new_tokens,
                temperature=self.config.generation.temperature,
                top_p=self.config.generation.top_p,
                top_k=self.config.generation.top_k,
                do_sample=self.config.generation.do_sample,
                # repetition_penalty=self.config.generation.repetition_penalty,
                stop_at_eos=True
            )
            
            # æ–°è¦ç”Ÿæˆã•ã‚ŒãŸéƒ¨åˆ†ã®ã¿ã‚’å–ã‚Šå‡ºã—ã¦ãƒ‡ã‚³ãƒ¼ãƒ‰
            new_tokens = generated_tokens[0, original_length:]  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å¤–
            response_text = self.model.to_string(new_tokens)
            num_response_tokens = new_tokens.shape[0]
            
            # === A. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®SAEæ´»æ€§åŒ–ã‚’å–å¾—ï¼ˆå¿œç­”ç”Ÿæˆç›´å‰ã®ã€Œæ„å›³ã€çŠ¶æ…‹ï¼‰ ===
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¿ã§ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’å®Ÿè¡Œ
            _, prompt_cache = self.model.run_with_cache(tokens)
            
            # å¯¾è±¡ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ•ãƒƒã‚¯åã‚’å–å¾—
            hook_name = self.config.model.hook_name
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ´»æ€§åŒ–ã‚’å–å¾—
            prompt_activations = prompt_cache[hook_name]  # shape: [batch, seq_len, d_model]
            
            # SAEã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
            prompt_sae_features = self.sae.encode(prompt_activations)  # shape: [batch, seq_len, n_features]
            
            # === B. å¿œç­”ã®æœ€åˆã®æ•°ãƒˆãƒ¼ã‚¯ãƒ³ã®SAEæ´»æ€§åŒ–ã‚’å–å¾—ï¼ˆè¿åˆçš„å¿œç­”ã®ã€Œå®Ÿè¡Œã€çŠ¶æ…‹ï¼‰ ===
            response_sae_features_list = []
            num_tokens_to_capture = min(
                self.feedback_config.response_tokens_to_capture,
                num_response_tokens
            )
            
            if num_tokens_to_capture > 0:
                # å¿œç­”ãƒˆãƒ¼ã‚¯ãƒ³ã‚’1ã¤ãšã¤è¿½åŠ ã—ãªãŒã‚‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’å®Ÿè¡Œ
                for i in range(num_tokens_to_capture):
                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + å¿œç­”ã®æœ€åˆã®i+1ãƒˆãƒ¼ã‚¯ãƒ³
                    tokens_with_response = generated_tokens[0, :original_length + i + 1].unsqueeze(0)
                    _, response_cache = self.model.run_with_cache(tokens_with_response)
                    
                    # å¿œç­”ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¾Œã®æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã®æ´»æ€§åŒ–ã‚’å–å¾—
                    response_activations = response_cache[hook_name][:, -1:, :]  # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿
                    response_sae_feature = self.sae.encode(response_activations)  # [1, 1, n_features]
                    response_sae_features_list.append(response_sae_feature[0, 0].cpu().numpy())
            
            # çµ±åˆ: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å¿œç­”ã®SAEç‰¹å¾´
            sae_features = prompt_sae_features  # æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®äº’æ›æ€§ã®ãŸã‚
            
            # === ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã®ä¿å­˜è¨­å®šã«å¿œã˜ã¦å‡¦ç† ===
            if self.feedback_config.save_all_tokens:
                # å…¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã®æ´»æ€§åŒ–ã‚’ä¿å­˜
                prompt_sae_activations_np = prompt_sae_features[0].cpu().numpy()  # [seq_len, n_features]
            else:
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ä¿å­˜ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€æ¨å¥¨ï¼‰
                # ã“ã‚ŒãŒå¿œç­”ã®æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆç›´å‰ã®çŠ¶æ…‹
                prompt_sae_activations_np = prompt_sae_features[0, -1:].cpu().numpy()  # [1, n_features]
            
            # æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®äº’æ›æ€§ã®ãŸã‚
            sae_activations_np = prompt_sae_activations_np
            
            # Top-kç‰¹å¾´ã‚’æŠ½å‡ºï¼ˆãƒ­ã‚°ãƒ»å¯è¦–åŒ–ç”¨ã€MLå­¦ç¿’ã«ã¯ä½¿ç”¨ã—ãªã„ï¼‰
            if self.feedback_config.save_all_tokens:
                # å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®å¹³å‡ã‚’å–ã‚‹
                mean_activations = sae_activations_np.mean(axis=0)
            else:
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ¨å¥¨ï¼‰
                mean_activations = sae_activations_np[0]
            
            top_k_indices = np.argsort(mean_activations)[-self.config.analysis.top_k_features:][::-1]
            top_k_features = [(int(idx), float(mean_activations[idx])) for idx in top_k_indices]
            
            # 0ã‚ˆã‚Šå¤§ãã„å…¨ã¦ã®æ´»æ€§åŒ–ã‚’ä¿å­˜ï¼ˆMLå­¦ç¿’ç”¨ã®ç–ãƒ™ã‚¯ãƒˆãƒ«ï¼‰
            # é‡è¦: é–¾å€¤ã«ã‚ˆã‚‹äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯è¡Œã‚ãšã€XGBoostã®ç‰¹å¾´é¸æŠã«å§”ã­ã‚‹
            active_features = {}
            
            if self.feedback_config.save_all_tokens:
                # å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã§ã®æ´»æ€§åŒ–ã‚’ä¿å­˜
                for token_idx in range(sae_activations_np.shape[0]):
                    token_activations = sae_activations_np[token_idx]
                    # 0ã‚ˆã‚Šå¤§ãã„å…¨ã¦ã®æ´»æ€§åŒ–ã‚’ä¿å­˜ï¼ˆç–ãƒ™ã‚¯ãƒˆãƒ«ï¼‰
                    active_indices = np.where(token_activations > 0)[0]
                    if len(active_indices) > 0:  # æ´»æ€§åŒ–ãŒã‚ã‚‹å ´åˆã®ã¿ä¿å­˜
                        active_features[f"token_{token_idx}"] = {
                            int(idx): float(token_activations[idx]) 
                            for idx in active_indices
                        }
            else:
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ï¼ˆæ¨å¥¨ã€è¿åˆæ€§åˆ†æã«æœ€é©ï¼‰
                token_activations = sae_activations_np[0]
                # 0ã‚ˆã‚Šå¤§ãã„å…¨ã¦ã®æ´»æ€§åŒ–ã‚’ä¿å­˜ï¼ˆç–ãƒ™ã‚¯ãƒˆãƒ«ï¼‰
                active_indices = np.where(token_activations > 0)[0]
                active_features["prompt_last_token"] = {
                    int(idx): float(token_activations[idx]) 
                    for idx in active_indices
                }
            
            # === å¿œç­”ã®æœ€åˆã®æ•°ãƒˆãƒ¼ã‚¯ãƒ³ã®SAEæ´»æ€§åŒ–ã‚’è¿½åŠ  ===
            for i, response_sae_np in enumerate(response_sae_features_list):
                # 0ã‚ˆã‚Šå¤§ãã„å…¨ã¦ã®æ´»æ€§åŒ–ã‚’ä¿å­˜ï¼ˆç–ãƒ™ã‚¯ãƒˆãƒ«ï¼‰
                active_indices = np.where(response_sae_np > 0)[0]
                if len(active_indices) > 0:
                    active_features[f"response_token_{i}"] = {
                        int(idx): float(response_sae_np[idx]) 
                        for idx in active_indices
                    }
            
            sae_info = {
                "hook_name": hook_name,
                "activations": active_features,  # 0ã‚ˆã‚Šå¤§ãã„å…¨æ´»æ€§åŒ–ï¼ˆç–ãƒ™ã‚¯ãƒˆãƒ«ã€MLå­¦ç¿’ç”¨ï¼‰
                "top_k_features": top_k_features,  # ãƒ­ã‚°ãƒ»å¯è¦–åŒ–ç”¨ï¼ˆMLå­¦ç¿’ã«ã¯ä¸ä½¿ç”¨ï¼‰
                "num_active_features": sum(len(v) for v in active_features.values()),
                "save_all_tokens": self.feedback_config.save_all_tokens,
                "num_tokens": sae_activations_np.shape[0],
                "analyzed_position": "prompt_last_token" if not self.feedback_config.save_all_tokens else "all_prompt_tokens",
                "response_tokens_captured": len(response_sae_features_list),  # å–å¾—ã—ãŸå¿œç­”ãƒˆãƒ¼ã‚¯ãƒ³æ•°
                "num_response_tokens": num_response_tokens,  # ç”Ÿæˆã•ã‚ŒãŸå¿œç­”ãƒˆãƒ¼ã‚¯ãƒ³ã®ç·æ•°
                "data_format": "sparse_vector",  # ãƒ‡ãƒ¼ã‚¿å½¢å¼: ç–ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæ´»æ€§åŒ–>0ã®ç‰¹å¾´ã®ã¿ä¿å­˜ï¼‰
                "total_sae_features": prompt_sae_features.shape[-1],  # SAEã®å…¨ç‰¹å¾´æ•°ï¼ˆä¾‹: 16384ï¼‰
                "capture_positions": {
                    "prompt_last_token": "å¿œç­”ç”Ÿæˆç›´å‰ã®æ„å›³ãƒ»è¨ˆç”»çŠ¶æ…‹",
                    "response_tokens": f"å¿œç­”ã®æœ€åˆã®{len(response_sae_features_list)}ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆè¿åˆçš„å¿œç­”ã®å®Ÿè¡Œãƒ»ç¶­æŒçŠ¶æ…‹ï¼‰" if response_sae_features_list else "å–å¾—ãªã—"
                }
            }
        
        return response_text, sae_info
    
    def analyze_prompt_variation(self, prompt_info: FeedbackPromptInfo) -> FeedbackResponse:
        """
        1ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åˆ†æ
        
        Args:
            prompt_info: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±
        
        Returns:
            FeedbackResponse ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        if self.config.debug.show_prompts:
            print(f"\nğŸ“ Prompt ({prompt_info.prompt_template_type}):")
            print(f"   {prompt_info.prompt[:100]}...")
        
        # ç”Ÿæˆå®Ÿè¡Œ
        start_time = datetime.now()
        response_text, sae_info = self.generate_with_sae(prompt_info.prompt)
        end_time = datetime.now()
        
        if self.config.debug.show_responses:
            print(f"ğŸ’¬ Response:")
            print(f"   {response_text[:200]}...")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = {
            "generation_time_ms": (end_time - start_time).total_seconds() * 1000,
            "response_length": len(response_text),
            "timestamp": datetime.now().isoformat()
        }
        
        if torch.cuda.is_available():
            metadata["gpu_memory_mb"] = torch.cuda.memory_allocated() / 1e6
        
        return FeedbackResponse(
            prompt_info=prompt_info,
            response_text=response_text,
            sae_activations=sae_info["activations"],
            top_k_features=sae_info["top_k_features"],
            metadata=metadata
        )
    
    def analyze_question_group(
        self, 
        question_id: int, 
        prompt_group: List[FeedbackPromptInfo]
    ) -> FeedbackQuestionResult:
        """
        1ã¤ã®è³ªå•ï¼ˆ5ã¤ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã‚’åˆ†æ
        
        Args:
            question_id: è³ªå•ID
            prompt_group: 5ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        
        Returns:
            FeedbackQuestionResult ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        if self.config.debug.verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š Analyzing Question {question_id} ({len(prompt_group)} variations)")
            print(f"{'='*60}")
        
        variations_results = []
        
        for prompt_info in prompt_group:
            response = self.analyze_prompt_variation(prompt_info)
            variations_results.append(response)
        
        # æœ€åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰åŸºæœ¬æƒ…å ±ã‚’å–å¾—
        first_prompt = prompt_group[0]
        base_text = first_prompt.base_data.get('text', '') or first_prompt.base_data.get('question', '')
        
        return FeedbackQuestionResult(
            question_id=question_id,
            dataset=first_prompt.dataset,
            base_text=base_text,
            variations=variations_results,
            timestamp=datetime.now().isoformat()
        )
    
    def run_analysis(self, sample_size: Optional[int] = None, start_index: Optional[int] = None, end_index: Optional[int] = None):
        """
        å®Œå…¨ãªåˆ†æã‚’å®Ÿè¡Œ
        
        Args:
            sample_size: åˆ†æã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆNoneã®å ´åˆã¯configã‹ã‚‰å–å¾—ï¼‰
            start_index: é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-basedã€Noneã®å ´åˆã¯0ã‹ã‚‰é–‹å§‹ï¼‰
            end_index: çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-basedã€Noneã®å ´åˆã¯æœ€å¾Œã¾ã§ï¼‰
        
        Note:
            - sample_sizeã¨start_index/end_indexã‚’åŒæ™‚ã«æŒ‡å®šã—ãŸå ´åˆã€start_index/end_indexãŒå„ªå…ˆã•ã‚Œã¾ã™
            - ä¾‹: start_index=100, end_index=500 ã§101å€‹ç›®ã‹ã‚‰500å€‹ç›®ã‚’å–å¾—ï¼ˆ0-indexedã®ãŸã‚ï¼‰
        """
        if self.config.debug.verbose:
            print("\n" + "="*60)
            print("ğŸš€ Starting Feedback Analysis")
            print("="*60)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
        feedback_data = self.load_feedback_data()
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        prompt_groups = self.aggregate_prompts(feedback_data)
        
        total_questions = len(prompt_groups)
        
        # ãƒ‡ãƒ¼ã‚¿ç¯„å›²ã®èª¿æ•´
        start = 0
        end = total_questions
        
        if start_index is not None or end_index is not None:
            # start_index/end_indexãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
            start = start_index if start_index is not None else 0
            end = end_index if end_index is not None else total_questions
            prompt_groups_to_process = prompt_groups[start:end]
            if self.config.debug.verbose:
                print(f"ğŸ“Š Analyzing questions {start+1} to {end} (total: {len(prompt_groups_to_process)} questions out of {total_questions})")
        else:
            # sample_sizeã«ã‚ˆã‚‹èª¿æ•´ï¼ˆå¾“æ¥ã®å‹•ä½œï¼‰
            if sample_size is None:
                sample_size = self.config.data.sample_size
            
            if sample_size is not None and sample_size < len(prompt_groups):
                prompt_groups_to_process = prompt_groups[:sample_size]
                end = sample_size
                if self.config.debug.verbose:
                    print(f"ğŸ“Š Analyzing {sample_size} questions (out of {total_questions} total)")
            else:
                prompt_groups_to_process = prompt_groups
        
        # ãƒ¢ãƒ‡ãƒ«ã¨SAEã®ãƒ­ãƒ¼ãƒ‰
        if self.model is None or self.sae is None:
            self.load_model_and_sae()
        
        # å‡¦ç†ç¯„å›²ã‚’è¨˜éŒ²
        self.processed_start_id = start
        self.processed_end_id = start  # åˆæœŸå€¤ã¯é–‹å§‹ä½ç½®
        
        # å„è³ªå•ã‚°ãƒ«ãƒ¼ãƒ—ã‚’åˆ†æ
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã«å…¨ä½“ã®å•é¡Œæ•°ã«å¯¾ã™ã‚‹é€²è¡ŒçŠ¶æ³ã‚’è¡¨ç¤º
        progress_desc = f"Processing questions ({start+1}-{end}/{total_questions})"
        try:
            for idx, prompt_group in enumerate(tqdm(prompt_groups_to_process, desc=progress_desc)):
                # å®Ÿéš›ã®è³ªå•IDã¯é–‹å§‹ä½ç½®ã‚’è€ƒæ…®
                actual_question_id = start + idx
                
                try:
                    result = self.analyze_question_group(actual_question_id, prompt_group)
                    self.results.append(result)
                    
                    # å‡¦ç†å®Œäº†ã—ãŸæœ€å¾Œã®question_idã‚’æ›´æ–°
                    self.processed_end_id = actual_question_id
                    
                    # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚’å®Ÿè¡Œ
                    if hasattr(self, 'optimize_memory_usage'):
                        self.optimize_memory_usage()
                    elif torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        
                except RuntimeError as e:
                    # CUDAãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ãªã©ã‚’ã‚­ãƒ£ãƒƒãƒ
                    if "out of memory" in str(e).lower() or "cuda" in str(e).lower():
                        print(f"\nâš ï¸ ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                        print(f"ğŸ’¾ Question ID {self.processed_start_id} ã‹ã‚‰ {self.processed_end_id} ã¾ã§ã®çµæœã‚’ä¿å­˜ã—ã¾ã™...")
                        # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã«ç¾åœ¨ã¾ã§ã®çµæœã‚’ä¿å­˜
                        self.save_results(error_recovery=True)
                        raise  # ã‚¨ãƒ©ãƒ¼ã‚’å†åº¦ç™ºç”Ÿã•ã›ã¦å‡¦ç†ã‚’åœæ­¢
                    else:
                        raise  # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã¯ãã®ã¾ã¾å†ç™ºç”Ÿ
        
        except Exception as e:
            # ãã®ä»–ã®äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ã‚‚ã‚­ãƒ£ãƒƒãƒã—ã¦ä¿å­˜
            if self.results:  # çµæœãŒã‚ã‚‹å ´åˆã®ã¿ä¿å­˜
                print(f"\nâš ï¸ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                print(f"ğŸ’¾ Question ID {self.processed_start_id} ã‹ã‚‰ {self.processed_end_id} ã¾ã§ã®çµæœã‚’ä¿å­˜ã—ã¾ã™...")
                self.save_results(error_recovery=True)
            raise
        
        if self.config.debug.verbose:
            print("\n" + "="*60)
            print("âœ… Analysis Complete")
            print("="*60)
            print(f"ğŸ“Š Processed {len(self.results)} questions")
            print(f"ğŸ’¾ Total variations: {sum(len(r.variations) for r in self.results)}")
            print(f"ğŸ¯ Question ID range: {self.processed_start_id} to {self.processed_end_id}")
    
    def save_results(self, output_path: Optional[str] = None, error_recovery: bool = False):
        """
        åˆ†æçµæœã‚’ä¿å­˜
        
        Args:
            output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
            error_recovery: ã‚¨ãƒ©ãƒ¼å›å¾©ãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹ï¼ˆãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ç­‰ã§é€”ä¸­ä¿å­˜ã™ã‚‹å ´åˆTrueï¼‰
        """
        if not self.results:
            print("âš ï¸ No results to save")
            return
        
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = self.config.model.name.replace("/", "_")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã«question_idç¯„å›²ã‚’è¿½åŠ 
            if self.processed_start_id is not None and self.processed_end_id is not None:
                range_str = f"{self.processed_start_id}-{self.processed_end_id}"
                prefix = "feedback_analysis_partial" if error_recovery else "feedback_analysis"
                output_path = self.results_dir / f"{prefix}_{model_name}_{timestamp}_{range_str}.json"
            else:
                output_path = self.results_dir / f"feedback_analysis_{model_name}_{timestamp}.json"
        
        # çµæœã‚’è¾æ›¸ã«å¤‰æ›
        output_data = {
            "metadata": {
                "model_name": self.config.model.name,
                "sae_release": self.config.model.sae_release,
                "sae_id": self.config.model.sae_id,
                "num_questions": len(self.results),
                "question_id_range": {
                    "start": self.processed_start_id,
                    "end": self.processed_end_id,
                    "total_processed": len(self.results)
                },
                "error_recovery": error_recovery,
                "save_all_tokens": self.feedback_config.save_all_tokens,
                "response_tokens_captured": self.feedback_config.response_tokens_to_capture,
                "analysis_position": {
                    "prompt": "prompt_last_token (å¿œç­”ç”Ÿæˆç›´å‰ã®æ„å›³)" if not self.feedback_config.save_all_tokens else "all_prompt_tokens",
                    "response": f"æœ€åˆã®{self.feedback_config.response_tokens_to_capture}ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆè¿åˆçš„å¿œç­”ã®å®Ÿè¡Œãƒ»ç¶­æŒï¼‰" if self.feedback_config.response_tokens_to_capture > 0 else "å–å¾—ãªã—"
                },
                "target_layer": self.feedback_config.target_layer,
                "timestamp": datetime.now().isoformat(),
                "config": {
                    "sample_size": self.config.data.sample_size,
                    "max_new_tokens": self.config.generation.max_new_tokens,
                    "temperature": self.config.generation.temperature,
                    "top_k_features": self.config.analysis.top_k_features
                }
            },
            "results": []
        }
        
        # å„è³ªå•ã®çµæœã‚’è¿½åŠ 
        for result in self.results:
            question_data = {
                "question_id": result.question_id,
                "dataset": result.dataset,
                "base_text": result.base_text[:200] + "..." if len(result.base_text) > 200 else result.base_text,
                "variations": []
            }
            
            for variation in result.variations:
                variation_data = {
                    "template_type": variation.prompt_info.prompt_template_type,
                    "prompt": variation.prompt_info.prompt if self.config.debug.show_prompts else "[hidden]",
                    "response": variation.response_text,
                    "sae_activations": variation.sae_activations,
                    "top_k_features": variation.top_k_features,
                    "metadata": variation.metadata
                }
                question_data["variations"].append(variation_data)
            
            output_data["results"].append(question_data)
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        if self.config.debug.verbose:
            print(f"\nğŸ’¾ Results saved to: {output_path}")
            file_size = os.path.getsize(output_path) / 1024 / 1024
            print(f"   ğŸ“¦ File size: {file_size:.2f} MB")
            if self.processed_start_id is not None and self.processed_end_id is not None:
                print(f"   ğŸ¯ Question ID range: {self.processed_start_id} to {self.processed_end_id}")
            if error_recovery:
                print(f"   âš ï¸ This is a partial save due to error recovery")
        
        return output_path
    
    def run_complete_analysis(self, sample_size: Optional[int] = None, start_index: Optional[int] = None, end_index: Optional[int] = None):
        """
        åˆ†æã®å®Ÿè¡Œã¨çµæœä¿å­˜ã‚’ä¸€æ‹¬ã§è¡Œã†
        
        Args:
            sample_size: åˆ†æã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
            start_index: é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-basedï¼‰
            end_index: çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-basedï¼‰
        """
        self.run_analysis(sample_size=sample_size, start_index=start_index, end_index=end_index)
        self.save_results()
        
        if self.config.debug.verbose:
            print("\nğŸ‰ Complete analysis finished!")
