"""
ä»‹å…¥å®Ÿé¨“å®Ÿè¡Œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (Step 5: Intervention & Evaluation)

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ç‰¹å®šã•ã‚ŒãŸSAEç‰¹å¾´é‡ã«å¯¾ã—ã¦Geometric Subtractionã«ã‚ˆã‚‹ä»‹å…¥ã‚’è¡Œã„ã€
ãã®åŠ¹æœã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®å®Ÿé¨“ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

ä¸»ãªæ©Ÿèƒ½:
- Zero-Ablation (Geometric Subtraction) ã«ã‚ˆã‚‹ç‰¹å¾´é‡é™¤å»
- Baseline (ä»‹å…¥ãªã—) vs Intervention (ä»‹å…¥ã‚ã‚Š) ã®æ¯”è¼ƒå®Ÿé¨“
- çµæœã®æ§‹é€ åŒ–ä¿å­˜ (Perplexityè¨ˆç®—ã¯å«ã¾ãªã„)
"""

import os
import gc
import json
import torch
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime
from tqdm import tqdm

# ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãŸã‚ã®ç’°å¢ƒå¤‰æ•°è¨­å®š
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
warnings.filterwarnings('ignore')

# SAE Lens imports
from transformer_lens import HookedTransformer
from sae_lens import SAE

# æ—¢å­˜ã®åˆ†æå™¨ã‹ã‚‰å¿…è¦ãªæ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from feedback_analyzer import FeedbackPromptInfo


@dataclass
class InterventionResult:
    """1ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã«å¯¾ã™ã‚‹ä»‹å…¥å®Ÿé¨“çµæœ"""
    prompt_info: FeedbackPromptInfo
    baseline_response: str
    intervention_response: str
    metadata: Dict[str, Any]


@dataclass
class QuestionInterventionResult:
    """1ã¤ã®è³ªå•ï¼ˆ5ã¤ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã®ä»‹å…¥å®Ÿé¨“çµæœ"""
    question_id: int
    dataset: str
    base_text: str
    variations: List[InterventionResult]
    timestamp: str


class InterventionRunner:
    """
    ä»‹å…¥å®Ÿé¨“å®Ÿè¡Œã‚¯ãƒ©ã‚¹
    
    FeedbackAnalyzerã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æ©Ÿèƒ½ã‚’å†åˆ©ç”¨ã—ã¤ã¤ã€
    ç‰¹å®šã•ã‚ŒãŸè¿åˆæ€§ç‰¹å¾´é‡ã«å¯¾ã™ã‚‹ä»‹å…¥å®Ÿé¨“ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    """
    
    def __init__(self, config, intervention_feature_ids: List[int]):
        """
        åˆæœŸåŒ–
        
        Args:
            config: ExperimentConfig ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            intervention_feature_ids: ä»‹å…¥å¯¾è±¡ã®ç‰¹å¾´é‡IDãƒªã‚¹ãƒˆ
        """
        self.config = config
        self.intervention_feature_ids = intervention_feature_ids
        self.model = None
        self.sae = None
        self.results: List[QuestionInterventionResult] = []
        
        # æ´»æ€§åŒ–åˆ†æç”¨ã®è¨˜éŒ²
        self.activation_stats: Dict[str, Any] = {
            'per_feature': {},  # ç‰¹å¾´é‡ã”ã¨ã®çµ±è¨ˆ
            'per_prompt': []    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã”ã¨ã®çµ±è¨ˆ
        }
        
        # ä»‹å…¥å°‚ç”¨è¨­å®šã®å–å¾—
        self.intervention_config = getattr(config, 'intervention', None)
        if self.intervention_config is None:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
            from config import InterventionConfig
            self.intervention_config = InterventionConfig()
        
        # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        self.results_dir = Path("results/intervention")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # å‡¦ç†ç¯„å›²ã‚’è¨˜éŒ²
        self.processed_start_id = None
        self.processed_end_id = None
        
        if self.config.debug.verbose:
            print("ğŸ”§ InterventionRunner initialized")
            print(f"   ğŸ“ Results directory: {self.results_dir}")
            print(f"   ğŸ¯ Target features: {len(intervention_feature_ids)} features")
            print(f"   ğŸ”¬ Intervention method: Geometric Subtraction (Zero-Ablation)")
            print(f"   âš™ï¸  Hook layer: {self.config.model.hook_name}")
    
    def optimize_memory_usage(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€é©åŒ–"""
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            gc.collect()
        except Exception as e:
            if self.config.debug.verbose:
                print(f"âš ï¸ Memory optimization warning: {e}")
    
    def load_feedback_data(self, data_path: Optional[str] = None) -> List[Dict]:
        """
        feedback.jsonlãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ (FeedbackAnalyzerã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)
        
        Args:
            data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯configã‹ã‚‰å–å¾—ï¼‰
        
        Returns:
            èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        if data_path is None:
            data_path = self.config.data.dataset_path
        
        if self.config.debug.verbose:
            print(f"ğŸ“‚ Loading feedback data from: {data_path}")
        
        with open(data_path, 'r', encoding='utf-8') as f:
            data = [json.loads(line) for line in f]
        
        if self.config.debug.verbose:
            print(f"âœ… Loaded {len(data)} entries")
        
        return data
    
    def create_prompt(self, data: Dict) -> FeedbackPromptInfo:
        """
        ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±ã‚’ä½œæˆ (FeedbackAnalyzerã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)
        
        Args:
            data: feedback.jsonlã®1ã‚¨ãƒ³ãƒˆãƒª
        
        Returns:
            FeedbackPromptInfo ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        dataset = data["base"]["dataset"]
        metadata = data["metadata"]
        prompt_template = metadata["prompt_template"]
        prompt_template_type = metadata["prompt_template_type"]
        
        if dataset == "arguments" or dataset == "poems":
            text = data["base"]["text"]
            prompt = prompt_template.format(text=text)
        elif dataset == "math":
            question = data["base"]["question"]
            correct_solution = data["base"]["correct_solution"]
            prompt = prompt_template.format(
                question=question, 
                correct_solution=correct_solution
            )
        else:
            raise ValueError(f"Unknown dataset: {dataset}")
        
        return FeedbackPromptInfo(
            dataset=dataset,
            prompt_template_type=prompt_template_type,
            prompt=prompt,
            base_data=data["base"]
        )
    
    def aggregate_prompts(self, feedback_data: List[Dict]) -> List[List[FeedbackPromptInfo]]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’5ã¤ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ– (FeedbackAnalyzerã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)
        
        Args:
            feedback_data: feedback.jsonlã®å…¨ãƒ‡ãƒ¼ã‚¿
        
        Returns:
            [[variation1, variation2, ..., variation5], ...] ã®å½¢å¼
        """
        prompt_variations = []
        prompt_groups = []
        
        for i, data in enumerate(feedback_data, 1):
            prompt_info = self.create_prompt(data)
            prompt_variations.append(prompt_info)
            
            # 5ã¤ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            if i % 5 == 0:
                prompt_groups.append(prompt_variations)
                prompt_variations = []
        
        # æ®‹ã‚ŠãŒã‚ã‚‹å ´åˆï¼ˆãƒ‡ãƒ¼ã‚¿ãŒ5ã®å€æ•°ã§ãªã„å ´åˆï¼‰
        if prompt_variations:
            prompt_groups.append(prompt_variations)
        
        if self.config.debug.verbose:
            print(f"ğŸ“¦ Grouped into {len(prompt_groups)} question sets")
        
        return prompt_groups
    
    def load_model_and_sae(self):
        """ãƒ¢ãƒ‡ãƒ«ã¨SAEã‚’ãƒ­ãƒ¼ãƒ‰"""
        if self.config.debug.verbose:
            print("ğŸ”„ Loading model and SAE...")
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        device = self.config.model.device
        if device == "auto":
            device = "cuda" if torch.cuda.is_available() else "cpu"
        
        self.device = device
        
        if self.config.debug.verbose:
            print(f"   ğŸ–¥ï¸  Using device: {device}")
        
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        if self.config.debug.verbose:
            print(f"   ğŸ“¥ Loading model: {self.config.model.name}")
        
        dtype = torch.bfloat16 if getattr(self.config.model, 'use_bfloat16', False) else torch.float16
        
        self.model = HookedTransformer.from_pretrained_no_processing(
            self.config.model.name,
            device=device,
            dtype=dtype
        )
        
        # SAEã®ãƒ­ãƒ¼ãƒ‰
        if self.config.debug.verbose:
            print(f"   ğŸ“¥ Loading SAE: {self.config.model.sae_release}/{self.config.model.sae_id}")
        
        self.sae, _, _ = SAE.from_pretrained(
            release=self.config.model.sae_release,
            sae_id=self.config.model.sae_id,
            device=device
        )
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å–å¾—
        self.tokenizer = self.model.tokenizer
        
        if self.config.debug.verbose:
            print("âœ… Model and SAE loaded successfully")
            if torch.cuda.is_available():
                print(f"   ğŸ’¾ GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB allocated")
    
    def _collect_activation_statistics(
        self, 
        sae_features: torch.Tensor, 
        masked_features: torch.Tensor, 
        activation_info: Dict[str, Any]
    ):
        """
        ãƒã‚¹ã‚¯ã—ãŸç‰¹å¾´é‡ã®æ´»æ€§åŒ–çµ±è¨ˆæƒ…å ±ã‚’åé›†
        
        Args:
            sae_features: å…¨SAEç‰¹å¾´é‡ã®æ´»æ€§å€¤ [batch, seq_len, n_features]
            masked_features: ãƒã‚¹ã‚¯é©ç”¨å¾Œã®ç‰¹å¾´é‡ [batch, seq_len, n_features]
            activation_info: çµ±è¨ˆæƒ…å ±ã‚’æ ¼ç´ã™ã‚‹è¾æ›¸ï¼ˆç ´å£Šçš„æ›´æ–°ï¼‰
        """
        # å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹å¾´é‡ã®çµ±è¨ˆã‚’è¨ˆç®—
        feature_stats = {}
        
        for feature_id in self.intervention_feature_ids:
            # è©²å½“ç‰¹å¾´é‡ã®æ´»æ€§å€¤ã‚’æŠ½å‡º [batch, seq_len]
            activations = sae_features[:, :, feature_id]
            
            # çµ±è¨ˆè¨ˆç®—ï¼ˆ0ã§ãªã„ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã®ã¿ï¼‰
            non_zero_mask = activations > 0
            non_zero_activations = activations[non_zero_mask]
            
            if len(non_zero_activations) > 0:
                feature_stats[str(feature_id)] = {
                    "mean": float(non_zero_activations.mean().item()),
                    "max": float(non_zero_activations.max().item()),
                    "min": float(non_zero_activations.min().item()),
                    "std": float(non_zero_activations.std().item()),
                    "num_active_tokens": int(non_zero_mask.sum().item()),
                    "total_tokens": int(activations.numel()),
                    "sparsity": float(non_zero_mask.sum().item() / activations.numel())
                }
            else:
                feature_stats[str(feature_id)] = {
                    "mean": 0.0,
                    "max": 0.0,
                    "min": 0.0,
                    "std": 0.0,
                    "num_active_tokens": 0,
                    "total_tokens": int(activations.numel()),
                    "sparsity": 0.0
                }
        
        # å…¨ä½“çµ±è¨ˆ
        all_masked_activations = masked_features[masked_features > 0]
        
        activation_info.update({
            "per_feature": feature_stats,
            "overall": {
                "mean_across_features": float(all_masked_activations.mean().item()) if len(all_masked_activations) > 0 else 0.0,
                "max_across_features": float(all_masked_activations.max().item()) if len(all_masked_activations) > 0 else 0.0,
                "total_active_features": int((masked_features > 0).sum().item()),
                "num_intervention_features": len(self.intervention_feature_ids)
            }
        })
    
    def get_activation_summary(self) -> Dict[str, Any]:
        """
        å®Ÿé¨“å…¨ä½“ã®æ´»æ€§åŒ–çµ±è¨ˆã‚µãƒãƒªã‚’å–å¾—
        
        Returns:
            å…¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ãŸã‚‹æ´»æ€§åŒ–çµ±è¨ˆã®é›†ç´„
        """
        if not self.results:
            return {"error": "No results available. Run experiment first."}
        
        # å„ç‰¹å¾´é‡ã®å…¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ãŸã‚‹çµ±è¨ˆã‚’é›†ç´„
        feature_aggregated = {}
        for feature_id in self.intervention_feature_ids:
            feature_id_str = str(feature_id)
            means = []
            maxs = []
            sparsities = []
            
            for question_result in self.results:
                for variation in question_result.variations:
                    stats = variation.metadata.get("activation_stats", {})
                    per_feature = stats.get("per_feature", {})
                    
                    if feature_id_str in per_feature:
                        means.append(per_feature[feature_id_str]["mean"])
                        maxs.append(per_feature[feature_id_str]["max"])
                        sparsities.append(per_feature[feature_id_str]["sparsity"])
            
            if means:
                feature_aggregated[feature_id_str] = {
                    "avg_mean_activation": float(sum(means) / len(means)),
                    "avg_max_activation": float(sum(maxs) / len(maxs)),
                    "avg_sparsity": float(sum(sparsities) / len(sparsities)),
                    "num_prompts": len(means)
                }
        
        return {
            "num_questions": len(self.results),
            "num_prompts": sum(len(q.variations) for q in self.results),
            "num_intervention_features": len(self.intervention_feature_ids),
            "per_feature_summary": feature_aggregated
        }
    
    def create_intervention_hook(self, collect_activations: bool = True):
        """
        Geometric Subtraction (Zero-Ablation) ã«ã‚ˆã‚‹ä»‹å…¥ãƒ•ãƒƒã‚¯ã‚’ä½œæˆ
        
        æ‰‹é †:
        1. æ®‹å·®ã‚¹ãƒˆãƒªãƒ¼ãƒ  x ã‚’ SAE ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹å¾´é‡ä»¥å¤–ã‚’ã™ã¹ã¦0ã«ãƒã‚¹ã‚¯
        3. ãƒã‚¹ã‚¯ã•ã‚ŒãŸæ´»æ€§å€¤ã‚’ä½¿ã£ã¦å†æ§‹æˆï¼ˆãƒã‚¤ã‚¢ã‚¹é …ãªã—ï¼‰
        4. å…ƒã®æ®‹å·®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‹ã‚‰æ¸›ç®—
        
        Args:
            collect_activations: æ´»æ€§åŒ–æƒ…å ±ã‚’åé›†ã™ã‚‹ã‹ã©ã†ã‹
        
        Returns:
            ãƒ•ãƒƒã‚¯é–¢æ•°ã€æ´»æ€§åŒ–çµ±è¨ˆæƒ…å ±ã®è¾æ›¸
        """
        activation_info = {}
        
        def intervention_hook(activations, hook):
            """
            Args:
                activations: æ®‹å·®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®ãƒ†ãƒ³ã‚½ãƒ« [batch, seq_len, d_model]
                hook: ãƒ•ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæƒ…å ±
            
            Returns:
                ä»‹å…¥å¾Œã®æ®‹å·®ã‚¹ãƒˆãƒªãƒ¼ãƒ 
            """
            with torch.no_grad():
                # 1. SAEã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ (å…¨ç‰¹å¾´é‡ã®æ´»æ€§å€¤ã‚’å–å¾—)
                sae_features = self.sae.encode(activations)  # [batch, seq_len, n_features]
                
                # 2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹å¾´é‡ã®ãƒã‚¹ã‚¯ã‚’ä½œæˆ
                # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹å¾´é‡ã®ã¿ã‚’1ã€ãã‚Œä»¥å¤–ã‚’0ã«ã™ã‚‹
                mask = torch.zeros_like(sae_features)
                for feature_id in self.intervention_feature_ids:
                    mask[:, :, feature_id] = 1.0
                
                # 3. ãƒã‚¹ã‚¯ã‚’é©ç”¨ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹å¾´é‡ã®ã¿ã‚’æ®‹ã™ï¼‰
                masked_features = sae_features * mask
                
                # æ´»æ€§åŒ–æƒ…å ±ã®åé›†
                if collect_activations:
                    self._collect_activation_statistics(
                        sae_features, 
                        masked_features, 
                        activation_info
                    )
                
                # 4. ãƒã‚¹ã‚¯ã•ã‚ŒãŸç‰¹å¾´é‡ã‹ã‚‰å†æ§‹æˆãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—ï¼ˆãƒã‚¤ã‚¢ã‚¹é …ã‚’é™¤å¤–ï¼‰
                # sae.decode()ã‚’ä½¿ã‚ãšã€W_decã¨ã®è¡Œåˆ—ç©ã®ã¿ã§å†æ§‹æˆ
                # reconstruction = masked_features @ W_dec.T
                reconstruction = torch.einsum(
                    "bsf,fd->bsd", 
                    masked_features, 
                    self.sae.W_dec
                )  # [batch, seq_len, d_model]
                
                # 5. å…ƒã®æ®‹å·®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‹ã‚‰æ¸›ç®— (Geometric Subtraction)
                intervened_activations = activations - reconstruction
                
                return intervened_activations
        
        return intervention_hook, activation_info
    
    def generate_baseline(self, prompt: str) -> str:
        """
        Baseline: ä»‹å…¥ãªã—ã§ã®é€šå¸¸ç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        with torch.no_grad():
            tokens = self.model.to_tokens(prompt)
            original_length = tokens.shape[1]
            
            # é€šå¸¸ã®ç”Ÿæˆï¼ˆãƒ•ãƒƒã‚¯ãªã—ï¼‰
            generated_tokens = self.model.generate(
                tokens,
                max_new_tokens=self.config.generation.max_new_tokens,
                temperature=self.config.generation.temperature,
                top_p=self.config.generation.top_p,
                top_k=self.config.generation.top_k,
                do_sample=self.config.generation.do_sample,
                stop_at_eos=True
            )
            
            # æ–°è¦ç”Ÿæˆéƒ¨åˆ†ã®ã¿ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
            new_tokens = generated_tokens[0, original_length:]
            response_text = self.model.to_string(new_tokens)
            
            return response_text
    
    def generate_with_intervention(self, prompt: str, collect_activations: bool = True) -> Tuple[str, Dict[str, Any]]:
        """
        Intervention: ä»‹å…¥ãƒ•ãƒƒã‚¯ã‚’é©ç”¨ã—ãŸçŠ¶æ…‹ã§ã®ç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            collect_activations: æ´»æ€§åŒ–æƒ…å ±ã‚’åé›†ã™ã‚‹ã‹ã©ã†ã‹
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€æ´»æ€§åŒ–çµ±è¨ˆæƒ…å ±ã®è¾æ›¸
        """
        with torch.no_grad():
            tokens = self.model.to_tokens(prompt)
            original_length = tokens.shape[1]
            
            # ä»‹å…¥ãƒ•ãƒƒã‚¯ã‚’ä½œæˆ
            hook_fn, activation_info = self.create_intervention_hook(collect_activations)
            hook_name = self.config.model.hook_name
            
            # ãƒ•ãƒƒã‚¯ã‚’é©ç”¨ã—ã¦ç”Ÿæˆ
            with self.model.hooks([(hook_name, hook_fn)]):
                generated_tokens = self.model.generate(
                    tokens,
                    max_new_tokens=self.config.generation.max_new_tokens,
                    temperature=self.config.generation.temperature,
                    top_p=self.config.generation.top_p,
                    top_k=self.config.generation.top_k,
                    do_sample=self.config.generation.do_sample,
                    stop_at_eos=True
                )
            
            # æ–°è¦ç”Ÿæˆéƒ¨åˆ†ã®ã¿ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
            new_tokens = generated_tokens[0, original_length:]
            response_text = self.model.to_string(new_tokens)
            
            return response_text, activation_info
    
    def analyze_prompt_variation(self, prompt_info: FeedbackPromptInfo) -> InterventionResult:
        """
        1ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã«å¯¾ã—ã¦ä»‹å…¥å®Ÿé¨“ã‚’å®Ÿè¡Œ
        
        Args:
            prompt_info: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±
        
        Returns:
            InterventionResult ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        if self.config.debug.show_prompts:
            print(f"\nğŸ“ Prompt ({prompt_info.prompt_template_type}): {prompt_info.prompt[:100]}...")
        
        # Baselineç”Ÿæˆ
        start_time = datetime.now()
        baseline_response = self.generate_baseline(prompt_info.prompt)
        baseline_time = (datetime.now() - start_time).total_seconds() * 1000
        
        if self.config.debug.show_responses:
            print(f"   ğŸ“¤ Baseline: {baseline_response}")
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        self.optimize_memory_usage()
        
        # Interventionç”Ÿæˆ
        start_time = datetime.now()
        intervention_response, activation_info = self.generate_with_intervention(prompt_info.prompt)
        intervention_time = (datetime.now() - start_time).total_seconds() * 1000
        
        if self.config.debug.show_responses:
            print(f"   ğŸ”¬ Intervention: {intervention_response}")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = {
            "baseline_generation_time_ms": baseline_time,
            "intervention_generation_time_ms": intervention_time,
            "baseline_response_length": len(baseline_response),
            "intervention_response_length": len(intervention_response),
            "timestamp": datetime.now().isoformat(),
            "activation_stats": activation_info  # æ´»æ€§åŒ–çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
        }
        
        if torch.cuda.is_available():
            metadata["gpu_memory_mb"] = torch.cuda.memory_allocated() / 1024**2
        
        return InterventionResult(
            prompt_info=prompt_info,
            baseline_response=baseline_response,
            intervention_response=intervention_response,
            metadata=metadata
        )
    
    def analyze_question_group(
        self, 
        question_id: int, 
        prompt_group: List[FeedbackPromptInfo]
    ) -> QuestionInterventionResult:
        """
        1ã¤ã®è³ªå•ï¼ˆ5ã¤ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã«å¯¾ã—ã¦ä»‹å…¥å®Ÿé¨“ã‚’å®Ÿè¡Œ
        
        Args:
            question_id: è³ªå•ID
            prompt_group: 5ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        
        Returns:
            QuestionInterventionResult ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        if self.config.debug.verbose:
            print(f"\n{'='*60}")
            print(f"Question {question_id}: Processing {len(prompt_group)} variations")
        
        variations_results = []
        
        for prompt_info in prompt_group:
            result = self.analyze_prompt_variation(prompt_info)
            variations_results.append(result)
            self.optimize_memory_usage()
        
        # æœ€åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰åŸºæœ¬æƒ…å ±ã‚’å–å¾—
        first_prompt = prompt_group[0]
        base_text = first_prompt.base_data.get('text', '') or first_prompt.base_data.get('question', '')
        
        return QuestionInterventionResult(
            question_id=question_id,
            dataset=first_prompt.dataset,
            base_text=base_text,
            variations=variations_results,
            timestamp=datetime.now().isoformat()
        )
    
    def run_intervention_experiment(
        self, 
        sample_size: Optional[int] = None, 
        start_index: Optional[int] = None, 
        end_index: Optional[int] = None
    ):
        """
        ä»‹å…¥å®Ÿé¨“ã‚’å®Ÿè¡Œ
        
        Args:
            sample_size: åˆ†æã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆNoneã®å ´åˆã¯configã‹ã‚‰å–å¾—ï¼‰
            start_index: é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-basedï¼‰
            end_index: çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-basedï¼‰
        """
        if self.config.debug.verbose:
            print("\n" + "="*60)
            print("ğŸš€ Starting Intervention Experiment")
            print("="*60)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
        feedback_data = self.load_feedback_data()
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        prompt_groups = self.aggregate_prompts(feedback_data)
        
        total_questions = len(prompt_groups)
        
        # ãƒ‡ãƒ¼ã‚¿ç¯„å›²ã®èª¿æ•´
        start = 0
        end = total_questions
        
        if start_index is not None or end_index is not None:
            start = start_index if start_index is not None else 0
            end = end_index if end_index is not None else total_questions
            end = min(end, total_questions)
            start = max(0, min(start, end))
        else:
            if sample_size is None:
                sample_size = self.config.data.sample_size
            if sample_size is not None:
                end = min(sample_size, total_questions)
        
        # ãƒ¢ãƒ‡ãƒ«ã¨SAEã®ãƒ­ãƒ¼ãƒ‰
        if self.model is None or self.sae is None:
            self.load_model_and_sae()
        
        # å‡¦ç†ç¯„å›²ã‚’è¨˜éŒ²
        self.processed_start_id = start
        self.processed_end_id = start
        
        # å„è³ªå•ã‚°ãƒ«ãƒ¼ãƒ—ã‚’åˆ†æ
        progress_desc = f"Processing questions ({start+1}-{end}/{total_questions})"
        try:
            for i in tqdm(range(start, end), desc=progress_desc):
                try:
                    prompt_group = prompt_groups[i]
                    result = self.analyze_question_group(i, prompt_group)
                    self.results.append(result)
                    self.processed_end_id = i
                    
                    # å®šæœŸçš„ã«ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                    if (i + 1) % 10 == 0:
                        self.optimize_memory_usage()
                        if self.config.debug.verbose:
                            print(f"\nğŸ’¾ Memory optimized at question {i+1}")
                
                except Exception as e:
                    print(f"\nâŒ Error processing question {i}: {e}")
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯é€”ä¸­çµæœã‚’ä¿å­˜
                    self.save_results(error_recovery=True)
                    raise
        
        except Exception as e:
            print(f"\nâŒ Fatal error during experiment: {e}")
            print(f"ğŸ“Š Processed {len(self.results)} questions before error")
            self.save_results(error_recovery=True)
            raise
        
        if self.config.debug.verbose:
            print(f"\nâœ… Experiment completed: {len(self.results)} questions processed")
    
    def save_results(self, output_path: Optional[str] = None, error_recovery: bool = False):
        """
        å®Ÿé¨“çµæœã‚’ä¿å­˜
        
        Args:
            output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
            error_recovery: ã‚¨ãƒ©ãƒ¼å›å¾©ãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹
        """
        if not self.results:
            print("âš ï¸ No results to save")
            return
        
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = self.config.model.name.replace("/", "-")
            range_suffix = f"{self.processed_start_id}-{self.processed_end_id}"
            filename = f"intervention_{model_name}_{timestamp}_{range_suffix}.json"
            if error_recovery:
                filename = f"intervention_{model_name}_{timestamp}_{range_suffix}_ERROR_RECOVERY.json"
            output_path = self.results_dir / filename
        
        # çµæœã‚’è¾æ›¸ã«å¤‰æ›
        output_data = {
            "metadata": {
                "model_name": self.config.model.name,
                "sae_release": self.config.model.sae_release,
                "sae_id": self.config.model.sae_id,
                "hook_name": self.config.model.hook_name,
                "intervention_method": "Geometric Subtraction (Zero-Ablation)",
                "num_intervention_features": len(self.intervention_feature_ids),
                "num_questions": len(self.results),
                "question_id_range": {
                    "start": self.processed_start_id,
                    "end": self.processed_end_id,
                    "total_processed": len(self.results)
                },
                "error_recovery": error_recovery,
                "timestamp": datetime.now().isoformat(),
                "config": {
                    "sample_size": self.config.data.sample_size,
                    "max_new_tokens": self.config.generation.max_new_tokens,
                    "temperature": self.config.generation.temperature,
                    "do_sample": self.config.generation.do_sample,
                    "top_p": self.config.generation.top_p,
                    "top_k": self.config.generation.top_k
                }
            },
            "intervention_features": self.intervention_feature_ids,
            "activation_summary": self.get_activation_summary(),  # æ´»æ€§åŒ–ã‚µãƒãƒªã‚’è¿½åŠ 
            "results": []
        }
        
        # å„è³ªå•ã®çµæœã‚’è¿½åŠ ï¼ˆSAE activationsã¯ä¿å­˜ã—ãªã„ï¼‰
        for result in self.results:
            question_data = {
                "question_id": result.question_id,
                "dataset": result.dataset,
                "base_text": result.base_text,
                "timestamp": result.timestamp,
                "variations": []
            }
            
            for variation in result.variations:
                variation_data = {
                    "template_type": variation.prompt_info.prompt_template_type,
                    "prompt": variation.prompt_info.prompt,
                    "baseline_response": variation.baseline_response,
                    "intervention_response": variation.intervention_response,
                    "metadata": variation.metadata
                }
                question_data["variations"].append(variation_data)
            
            output_data["results"].append(question_data)
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        if self.config.debug.verbose:
            print(f"\nğŸ’¾ Results saved to: {output_path}")
            print(f"   ğŸ“Š Questions: {len(self.results)}")
            print(f"   ğŸ¯ Intervention features: {len(self.intervention_feature_ids)}")
            print(f"   ğŸ“ File size: {output_path.stat().st_size / 1024:.2f} KB")
        
        return output_path
    
    def run_complete_experiment(
        self, 
        sample_size: Optional[int] = None, 
        start_index: Optional[int] = None, 
        end_index: Optional[int] = None
    ):
        """
        å®Ÿé¨“ã®å®Ÿè¡Œã¨çµæœä¿å­˜ã‚’ä¸€æ‹¬ã§è¡Œã†
        
        Args:
            sample_size: åˆ†æã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
            start_index: é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-basedï¼‰
            end_index: çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-basedï¼‰
        """
        self.run_intervention_experiment(
            sample_size=sample_size, 
            start_index=start_index, 
            end_index=end_index
        )
        output_path = self.save_results()
        
        if self.config.debug.verbose:
            print("\n" + "="*60)
            print("âœ… Complete experiment finished successfully")
            print("="*60)
        
        return output_path
