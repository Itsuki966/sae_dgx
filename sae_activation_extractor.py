"""
SAE Activation Extractor
Teacher Forcingã‚’åˆ©ç”¨ã—ã¦æŒ‡å®šãƒ¬ã‚¤ãƒ¤ãƒ¼ã®SAE activationã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¹
"""

import torch
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
from transformer_lens import HookedTransformer
from sae_lens import SAE
from dataclasses import dataclass, asdict
import json
import os
import gc
from pathlib import Path
from datetime import datetime
from tqdm import tqdm


@dataclass
class ExtractionConfig:
    """SAE ActivationæŠ½å‡ºã®è¨­å®š"""
    model_name: str
    sae_release: str
    sae_id: str
    target_layer: int
    hook_name: str
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    dtype: torch.dtype = torch.bfloat16
    top_k_features: int = 50  # ä¿å­˜ã™ã‚‹ä¸Šä½ç‰¹å¾´æ•°


@dataclass
class PromptInfo:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±ï¼ˆfeedback_analyzer.pyã®FeedbackPromptInfoç›¸å½“ï¼‰"""
    dataset: str
    prompt_template_type: str
    prompt: str
    base_data: Dict[str, Any]


@dataclass
class ExtractionResponse:
    """1ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹å¿œç­”ã¨SAEçŠ¶æ…‹ï¼ˆfeedback_analyzer.pyã®FeedbackResponseç›¸å½“ï¼‰"""
    prompt_info: PromptInfo
    response_text: str
    sae_activations: Dict[str, Any]  # {token_key: {feature_id: activation_value}}
    top_k_features: List[Tuple[int, float]]  # [(feature_id, value), ...]
    metadata: Dict[str, Any]


@dataclass
class QuestionResult:
    """1ã¤ã®è³ªå•ã®åˆ†æçµæœï¼ˆfeedback_analyzer.pyã®FeedbackQuestionResultç›¸å½“ï¼‰"""
    question_id: int
    dataset: str
    base_text: str
    variations: List[ExtractionResponse]
    timestamp: str


class SAEActivationExtractor:
    """
    Teacher Forcingã‚’ä½¿ç”¨ã—ã¦SAE activationã‚’æŠ½å‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹
    
    ä½¿ç”¨ä¾‹:
        config = ExtractionConfig(
            model_name="google/gemma-2-9b-it",
            sae_release="gemma-scope-9b-pt-res-canonical",
            sae_id="layer_20/width_16k/canonical",
            target_layer=20,
            hook_name="blocks.20.hook_resid_post"
        )
        extractor = SAEActivationExtractor(config)
        result = extractor.extract_activations(prompt, response)
    """
    
    def __init__(self, config: ExtractionConfig):
        self.config = config
        self.model: Optional[HookedTransformer] = None
        self.sae: Optional[SAE] = None
        self.results: List[QuestionResult] = []
        self.save_all_tokens: bool = False  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿
        
        # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        self.results_dir = Path("results/feedback")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
    def load_model_and_sae(self):
        """ãƒ¢ãƒ‡ãƒ«ã¨SAEã‚’ãƒ­ãƒ¼ãƒ‰"""
        print(f"ğŸ”„ Loading model: {self.config.model_name}")
        self.model = HookedTransformer.from_pretrained_no_processing(
            self.config.model_name,
            device=self.config.device,
            dtype=self.config.dtype
        )
        
        print(f"ğŸ”„ Loading SAE: {self.config.sae_release}/{self.config.sae_id}")
        self.sae = SAE.from_pretrained(
            release=self.config.sae_release,
            sae_id=self.config.sae_id,
            device=self.config.device
        )
        
        print(f"âœ… Model and SAE loaded successfully")
        print(f"   Target Layer: {self.config.target_layer}")
        print(f"   Hook Name: {self.config.hook_name}")
        
    def _find_answer_start_position(self, full_tokens: torch.Tensor, prompt_str: str) -> int:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®çµ‚ã‚ã‚Šï¼ˆå›ç­”ã®å§‹ã¾ã‚Šï¼‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã‚’ç‰¹å®š
        
        Args:
            full_tokens: ãƒ•ãƒ«ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒ³
            prompt_str: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
            
        Returns:
            å›ç­”é–‹å§‹ä½ç½®ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-indexedï¼‰
        """
        prompt_tokens = self.model.to_tokens(prompt_str, prepend_bos=True)
        return prompt_tokens.shape[1] - 1
    
    def extract_activations(
        self,
        prompt: str,
        response: str
    ) -> Tuple[Dict[str, Any], List[Tuple[int, float]]]:
        """
        Teacher Forcingã‚’ä½¿ç”¨ã—ã¦SAE activationã‚’æŠ½å‡º
        feedback_analyzer.pyã®generate_with_saeã¨åŒã˜å½¢å¼ã§è¿”ã™
        
        Args:
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
            response: å¿œç­”æ–‡å­—åˆ—ï¼ˆæ—¢ã«ç”Ÿæˆæ¸ˆã¿ï¼‰
                           
        Returns:
            (sae_activations, top_k_features)ã®ã‚¿ãƒ—ãƒ«
            - sae_activations: {token_key: {feature_id: activation_value}}ã®ç–ãƒ™ã‚¯ãƒˆãƒ«å½¢å¼
            - top_k_features: [(feature_id, value), ...]ã®ãƒªã‚¹ãƒˆ
        """
        if self.model is None or self.sae is None:
            raise RuntimeError("Model and SAE must be loaded first. Call load_model_and_sae().")
        
        # Teacher Forcing Input: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + å®Ÿéš›ã®å¿œç­”
        full_text = prompt + response
        input_tokens = self.model.to_tokens(full_text, prepend_bos=True)
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ãƒã‚§ãƒƒã‚¯
        if input_tokens.shape[1] > self.model.cfg.n_ctx:
            raise ValueError(f"Sequence too long: {input_tokens.shape[1]} > {self.model.cfg.n_ctx}")
        
        # å›ç­”é–‹å§‹ä½ç½®ã‚’ç‰¹å®š
        answer_start_pos = self._find_answer_start_position(input_tokens, prompt)
        
        # Forward Passã§activationã‚’å–å¾—
        self.model.eval()
        with torch.no_grad():
            _, cache = self.model.run_with_cache(input_tokens)
            
            # å¯¾è±¡ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®activationã‚’å–å¾—
            activations = cache[self.config.hook_name]  # [batch, seq, d_model]
            
            # SAEã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            sae_features = self.sae.encode(activations)  # [batch, seq, n_features]
            
            # NumPyé…åˆ—ã«å¤‰æ›
            if self.save_all_tokens:
                # å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®activationã‚’ä¿å­˜
                sae_activations_np = sae_features[0].cpu().numpy()  # [seq_len, n_features]
            else:
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå¿œç­”ç›´å‰ï¼‰ã®ã¿ä¿å­˜ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€æ¨å¥¨ï¼‰
                sae_activations_np = sae_features[0, answer_start_pos:answer_start_pos+1].cpu().numpy()  # [1, n_features]
            
            # Top-kç‰¹å¾´ã‚’æŠ½å‡ºï¼ˆãƒ­ã‚°ãƒ»å¯è¦–åŒ–ç”¨ï¼‰
            if self.save_all_tokens:
                mean_activations = sae_activations_np.mean(axis=0)
            else:
                mean_activations = sae_activations_np[0]
            
            top_k_indices = np.argsort(mean_activations)[-self.config.top_k_features:][::-1]
            top_k_features = [(int(idx), float(mean_activations[idx])) for idx in top_k_indices]
            
            # 0ã‚ˆã‚Šå¤§ãã„å…¨ã¦ã®æ´»æ€§åŒ–ã‚’ä¿å­˜ï¼ˆç–ãƒ™ã‚¯ãƒˆãƒ«å½¢å¼ï¼‰
            active_features = {}
            
            if self.save_all_tokens:
                # å„ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã§ã®æ´»æ€§åŒ–ã‚’ä¿å­˜
                for token_idx in range(sae_activations_np.shape[0]):
                    token_activations = sae_activations_np[token_idx]
                    active_indices = np.where(token_activations > 0)[0]
                    if len(active_indices) > 0:
                        active_features[f"token_{token_idx}"] = {
                            int(idx): float(token_activations[idx]) 
                            for idx in active_indices
                        }
            else:
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ï¼ˆæ¨å¥¨ï¼‰
                token_activations = sae_activations_np[0]
                active_indices = np.where(token_activations > 0)[0]
                active_features["prompt_last_token"] = {
                    int(idx): float(token_activations[idx]) 
                    for idx in active_indices
                }
        
        return active_features, top_k_features
    
    def analyze_sample(self, prompt_info: PromptInfo, response_text: str) -> ExtractionResponse:
        """
        1ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’åˆ†æï¼ˆfeedback_analyzer.pyã®analyze_prompt_variationã¨åŒã˜ï¼‰
        
        Args:
            prompt_info: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±
            response_text: å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ—¢ã«ç”Ÿæˆæ¸ˆã¿ï¼‰
        
        Returns:
            ExtractionResponse ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        # SAE activationã‚’æŠ½å‡º
        start_time = datetime.now()
        sae_activations, top_k_features = self.extract_activations(
            prompt_info.prompt, 
            response_text
        )
        end_time = datetime.now()
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = {
            "extraction_time_ms": (end_time - start_time).total_seconds() * 1000,
            "response_length": len(response_text),
            "timestamp": datetime.now().isoformat()
        }
        
        if torch.cuda.is_available():
            metadata["gpu_memory_mb"] = torch.cuda.memory_allocated() / 1e6
        
        return ExtractionResponse(
            prompt_info=prompt_info,
            response_text=response_text,
            sae_activations=sae_activations,
            top_k_features=top_k_features,
            metadata=metadata
        )
    
    def analyze_question_group(
        self,
        question_id: int,
        dataset: str,
        base_text: str,
        variations: List[Dict[str, Any]],
        verbose: bool = True
    ) -> QuestionResult:
        """
        1ã¤ã®è³ªå•ï¼ˆè¤‡æ•°ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã‚’åˆ†æ
        
        Args:
            question_id: è³ªå•ID
            dataset: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
            base_text: ãƒ™ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆ
            variations: ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒªã‚¹ãƒˆ
            verbose: è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹ã‹
        
        Returns:
            QuestionResult ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        if verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š Analyzing Question {question_id} ({len(variations)} variations)")
            print(f"{'='*60}")
        
        variation_results = []
        
        for variation in variations:
            # PromptInfoã‚’ä½œæˆ
            prompt_info = PromptInfo(
                dataset=dataset,
                prompt_template_type=variation.get("template_type", "(base)"),
                prompt=variation.get("prompt", ""),
                base_data={"text": base_text}
            )
            
            # åˆ†æå®Ÿè¡Œ
            response = self.analyze_sample(
                prompt_info=prompt_info,
                response_text=variation.get("response", "")
            )
            variation_results.append(response)
            
            if verbose:
                print(f"   âœ… {prompt_info.prompt_template_type}: {len(response.sae_activations)} token positions")
        
        return QuestionResult(
            question_id=question_id,
            dataset=dataset,
            base_text=base_text,
            variations=variation_results,
            timestamp=datetime.now().isoformat()
        )
    
    def run_extraction(
        self,
        input_json_path: str,
        sample_size: Optional[int] = None,
        save_all_tokens: bool = False,
        verbose: bool = True
    ):
        """
        åˆ†æã‚’å®Ÿè¡Œï¼ˆfeedback_analyzer.pyã®run_analysisã¨åŒã˜ï¼‰
        
        Args:
            input_json_path: å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            sample_size: å‡¦ç†ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰
            save_all_tokens: å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®activationã‚’ä¿å­˜ã™ã‚‹ã‹
            verbose: è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹ã‹
        """
        self.save_all_tokens = save_all_tokens
        
        if verbose:
            print("\n" + "="*60)
            print("ğŸš€ Starting SAE Activation Extraction")
            print("="*60)
            print(f"   Model: {self.config.model_name}")
            print(f"   Target Layer: {self.config.target_layer}")
            print(f"   SAE ID: {self.config.sae_id}")
            print(f"   Save All Tokens: {save_all_tokens}")
        
        # å…¥åŠ›JSONã‚’èª­ã¿è¾¼ã¿
        with open(input_json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        results = data.get("results", [])
        total_questions = len(results)
        
        # ã‚µãƒ³ãƒ—ãƒ«æ•°ã®èª¿æ•´
        if sample_size is not None and sample_size < total_questions:
            results = results[:sample_size]
            if verbose:
                print(f"   ğŸ“Š Processing {sample_size} questions (out of {total_questions})")
        else:
            if verbose:
                print(f"   ğŸ“Š Processing all {total_questions} questions")
        
        # ãƒ¢ãƒ‡ãƒ«ã¨SAEã®ãƒ­ãƒ¼ãƒ‰
        if self.model is None or self.sae is None:
            self.load_model_and_sae()
        
        # å„è³ªå•ã‚°ãƒ«ãƒ¼ãƒ—ã‚’åˆ†æ
        for result in tqdm(results, desc="Processing questions"):
            question_id = result.get("question_id")
            dataset = result.get("dataset", "unknown")
            base_text = result.get("base_text", "")
            variations = result.get("variations", [])
            
            try:
                question_result = self.analyze_question_group(
                    question_id=question_id,
                    dataset=dataset,
                    base_text=base_text,
                    variations=variations,
                    verbose=False  # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä½¿ç”¨æ™‚ã¯å€‹åˆ¥ãƒ­ã‚°ã‚’æŠ‘åˆ¶
                )
                self.results.append(question_result)
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                if verbose:
                    print(f"\nâš ï¸ Error processing question {question_id}: {e}")
                continue
        
        if verbose:
            print("\n" + "="*60)
            print("âœ… Extraction Complete")
            print("="*60)
            print(f"ğŸ“Š Processed {len(self.results)} questions")
            print(f"ğŸ’¾ Total variations: {sum(len(r.variations) for r in self.results)}")
    
    def save_results(self, output_path: Optional[str] = None):
        """
        åˆ†æçµæœã‚’ä¿å­˜ï¼ˆfeedback_analyzer.pyã®save_resultsã¨åŒã˜å½¢å¼ï¼‰
        
        Args:
            output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        """
        if not self.results:
            print("âš ï¸ No results to save")
            return
        
        if output_path is None:
            # ãƒ•ã‚¡ã‚¤ãƒ«å: model_layer{XX}_{position}_YYYYMMDD_HHMMSS.json
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = self.config.model_name.replace("/", "_").replace("-", "_")
            position = "all_tokens" if self.save_all_tokens else "last_token"
            output_path = self.results_dir / f"{model_name}_layer{self.config.target_layer}_{position}_{timestamp}.json"
        
        # çµæœã‚’è¾æ›¸ã«å¤‰æ›ï¼ˆfeedback_analyzer.pyã¨åŒã˜å½¢å¼ï¼‰
        output_data = {
            "metadata": {
                "model_name": self.config.model_name,
                "sae_release": self.config.sae_release,
                "sae_id": self.config.sae_id,
                "target_layer": self.config.target_layer,
                "hook_name": self.config.hook_name,
                "num_questions": len(self.results),
                "save_all_tokens": self.save_all_tokens,
                "analyzed_position": "all_prompt_tokens" if self.save_all_tokens else "prompt_last_token",
                "timestamp": datetime.now().isoformat(),
                "config": {
                    "top_k_features": self.config.top_k_features,
                    "device": self.config.device,
                    "dtype": str(self.config.dtype)
                }
            },
            "results": []
        }
        
        # å„è³ªå•ã®çµæœã‚’è¿½åŠ 
        for result in self.results:
            question_data = {
                "question_id": result.question_id,
                "dataset": result.dataset,
                "base_text": result.base_text[:200] + "..." if len(result.base_text) > 200 else result.base_text,
                "variations": []
            }
            
            for variation in result.variations:
                variation_data = {
                    "template_type": variation.prompt_info.prompt_template_type,
                    "prompt": variation.prompt_info.prompt,
                    "response": variation.response_text,
                    "sae_activations": variation.sae_activations,
                    "top_k_features": variation.top_k_features,
                    "metadata": variation.metadata
                }
                question_data["variations"].append(variation_data)
            
            output_data["results"].append(question_data)
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Results saved to: {output_path}")
        file_size = os.path.getsize(output_path) / 1024 / 1024
        print(f"   ğŸ“¦ File size: {file_size:.2f} MB")
        
        return output_path
    
    def run_complete_extraction(
        self,
        input_json_path: str,
        sample_size: Optional[int] = None,
        save_all_tokens: bool = False,
        verbose: bool = True
    ):
        """
        æŠ½å‡ºã®å®Ÿè¡Œã¨çµæœä¿å­˜ã‚’ä¸€æ‹¬ã§è¡Œã†
        
        Args:
            input_json_path: å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            sample_size: å‡¦ç†ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
            save_all_tokens: å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®activationã‚’ä¿å­˜ã™ã‚‹ã‹
            verbose: è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹ã‹
        """
        self.run_extraction(
            input_json_path=input_json_path,
            sample_size=sample_size,
            save_all_tokens=save_all_tokens,
            verbose=verbose
        )
        self.save_results()
        
        if verbose:
            print("\nğŸ‰ Complete extraction finished!")
    
    def cleanup(self):
        """ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.model is not None:
            del self.model
            self.model = None
        if self.sae is not None:
            del self.sae
            self.sae = None
        torch.cuda.empty_cache()
        print("âœ… Memory cleaned up")
