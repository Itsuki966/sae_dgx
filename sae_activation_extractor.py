"""
SAE Activation Extractor
Teacher Forcingã‚’åˆ©ç”¨ã—ã¦æŒ‡å®šãƒ¬ã‚¤ãƒ¤ãƒ¼ã®SAE activationã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¹
"""

import torch
from typing import Dict, Any, List, Optional, Tuple
from transformer_lens import HookedTransformer
from sae_lens import SAE
from dataclasses import dataclass
import json
from pathlib import Path


@dataclass
class ExtractionConfig:
    """SAE ActivationæŠ½å‡ºã®è¨­å®š"""
    model_name: str
    sae_release: str
    sae_id: str
    target_layer: int
    hook_name: str
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    dtype: torch.dtype = torch.bfloat16
    top_k_features: int = 50  # ä¿å­˜ã™ã‚‹ä¸Šä½ç‰¹å¾´æ•°


class SAEActivationExtractor:
    """
    Teacher Forcingã‚’ä½¿ç”¨ã—ã¦SAE activationã‚’æŠ½å‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹
    
    ä½¿ç”¨ä¾‹:
        config = ExtractionConfig(
            model_name="google/gemma-2-9b-it",
            sae_release="gemma-scope-9b-pt-res-canonical",
            sae_id="layer_20/width_16k/canonical",
            target_layer=20,
            hook_name="blocks.20.hook_resid_post"
        )
        extractor = SAEActivationExtractor(config)
        result = extractor.extract_activations(prompt, response)
    """
    
    def __init__(self, config: ExtractionConfig):
        self.config = config
        self.model: Optional[HookedTransformer] = None
        self.sae: Optional[SAE] = None
        
    def load_model_and_sae(self):
        """ãƒ¢ãƒ‡ãƒ«ã¨SAEã‚’ãƒ­ãƒ¼ãƒ‰"""
        print(f"ğŸ”„ Loading model: {self.config.model_name}")
        self.model = HookedTransformer.from_pretrained_no_processing(
            self.config.model_name,
            device=self.config.device,
            dtype=self.config.dtype
        )
        
        print(f"ğŸ”„ Loading SAE: {self.config.sae_release}/{self.config.sae_id}")
        self.sae = SAE.from_pretrained(
            release=self.config.sae_release,
            sae_id=self.config.sae_id,
            device=self.config.device
        )
        
        print(f"âœ… Model and SAE loaded successfully")
        print(f"   Target Layer: {self.config.target_layer}")
        print(f"   Hook Name: {self.config.hook_name}")
        
    def _find_answer_start_position(self, full_tokens: torch.Tensor, prompt_str: str) -> int:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®çµ‚ã‚ã‚Šï¼ˆå›ç­”ã®å§‹ã¾ã‚Šï¼‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã‚’ç‰¹å®š
        
        Args:
            full_tokens: ãƒ•ãƒ«ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒ³
            prompt_str: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
            
        Returns:
            å›ç­”é–‹å§‹ä½ç½®ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-indexedï¼‰
        """
        prompt_tokens = self.model.to_tokens(prompt_str, prepend_bos=True)
        return prompt_tokens.shape[1] - 1
    
    def extract_activations(
        self,
        prompt: str,
        response: str,
        save_all_tokens: bool = False
    ) -> Dict[str, Any]:
        """
        Teacher Forcingã‚’ä½¿ç”¨ã—ã¦SAE activationã‚’æŠ½å‡º
        
        Args:
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
            response: å¿œç­”æ–‡å­—åˆ—
            save_all_tokens: Trueã®å ´åˆã€å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®activationã‚’ä¿å­˜
                           Falseã®å ´åˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå¿œç­”ç›´å‰ï¼‰ã®ã¿ä¿å­˜
                           
        Returns:
            æŠ½å‡ºçµæœã‚’å«ã‚€è¾æ›¸
        """
        if self.model is None or self.sae is None:
            raise RuntimeError("Model and SAE must be loaded first. Call load_model_and_sae().")
        
        # Teacher Forcing Input: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + å®Ÿéš›ã®å¿œç­”
        full_text = prompt + response
        input_tokens = self.model.to_tokens(full_text, prepend_bos=True)
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ãƒã‚§ãƒƒã‚¯
        if input_tokens.shape[1] > self.model.cfg.n_ctx:
            return {
                "status": "error",
                "error": f"Sequence too long: {input_tokens.shape[1]} > {self.model.cfg.n_ctx}"
            }
        
        # å›ç­”é–‹å§‹ä½ç½®ã‚’ç‰¹å®š
        answer_start_pos = self._find_answer_start_position(input_tokens, prompt)
        
        # Activationã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®è¾æ›¸
        activation_storage = {}
        
        def capture_hook(activation, hook):
            """
            Activationã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹ãƒ•ãƒƒã‚¯
            activation: [batch, seq, d_model]
            """
            if save_all_tokens:
                # å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®activationã‚’ä¿å­˜
                for pos in range(activation.shape[1]):
                    act = activation[:, pos:pos+1, :]
                    f_acts = self.sae.encode(act)  # [1, 1, n_features]
                    activation_storage[f"token_{pos}"] = f_acts.detach().cpu()
            else:
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå¿œç­”ç›´å‰ï¼‰ã®ã¿ä¿å­˜
                target_act = activation[:, answer_start_pos:answer_start_pos+1, :]
                f_acts = self.sae.encode(target_act)  # [1, 1, n_features]
                activation_storage["last_token"] = f_acts.detach().cpu()
            
            return activation
        
        # Forward Pass with Hook
        self.model.eval()
        with torch.no_grad():
            _ = self.model.run_with_hooks(
                input_tokens,
                fwd_hooks=[(self.config.hook_name, capture_hook)]
            )
        
        # çµæœã‚’æ•´å½¢
        result = {
            "status": "success",
            "answer_start_position": answer_start_pos,
            "total_tokens": input_tokens.shape[1],
            "prompt_length": len(prompt),
            "response_length": len(response),
            "activations": {}
        }
        
        # Top-kç‰¹å¾´ã‚’æŠ½å‡ºã—ã¦ä¿å­˜
        for token_key, f_acts in activation_storage.items():
            f_acts_flat = f_acts.squeeze()  # [n_features]
            
            # Top-kç‰¹å¾´ã‚’å–å¾—
            top_values, top_indices = torch.topk(f_acts_flat, k=self.config.top_k_features)
            
            # éã‚¼ãƒ­ã®ç‰¹å¾´ã®ã¿ã‚’ä¿å­˜
            top_features = []
            for idx, val in zip(top_indices, top_values):
                if val.item() > 0:
                    top_features.append({
                        "id": int(idx.item()),
                        "activation": float(val.item())
                    })
            
            result["activations"][token_key] = {
                "top_k_features": top_features,
                "num_active_features": len(top_features)
            }
        
        return result
    
    def extract_batch(
        self,
        samples: List[Dict[str, Any]],
        save_all_tokens: bool = False,
        verbose: bool = True
    ) -> List[Dict[str, Any]]:
        """
        è¤‡æ•°ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã—ã¦ãƒãƒƒãƒå‡¦ç†
        
        Args:
            samples: ã‚µãƒ³ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆã€‚å„ã‚µãƒ³ãƒ—ãƒ«ã¯ {'prompt': str, 'response': str, ...} ã®å½¢å¼
            save_all_tokens: å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®activationã‚’ä¿å­˜ã™ã‚‹ã‹
            verbose: é€²æ—ã‚’è¡¨ç¤ºã™ã‚‹ã‹
            
        Returns:
            å„ã‚µãƒ³ãƒ—ãƒ«ã®æŠ½å‡ºçµæœã®ãƒªã‚¹ãƒˆ
        """
        results = []
        
        for i, sample in enumerate(samples):
            if verbose and (i + 1) % 10 == 0:
                print(f"   Processing sample {i + 1}/{len(samples)}...")
            
            try:
                result = self.extract_activations(
                    prompt=sample['prompt'],
                    response=sample['response'],
                    save_all_tokens=save_all_tokens
                )
                
                # å…ƒã®ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ã‚’ä¿æŒ
                result.update({
                    "question_id": sample.get("question_id"),
                    "template_type": sample.get("template_type"),
                    "variation_index": sample.get("variation_index")
                })
                
                results.append(result)
                
            except Exception as e:
                results.append({
                    "status": "error",
                    "error": str(e),
                    "question_id": sample.get("question_id"),
                    "template_type": sample.get("template_type")
                })
                if verbose:
                    print(f"   âš ï¸ Error processing sample {i}: {e}")
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if (i + 1) % 20 == 0:
                torch.cuda.empty_cache()
        
        return results
    
    def cleanup(self):
        """ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.model is not None:
            del self.model
            self.model = None
        if self.sae is not None:
            del self.sae
            self.sae = None
        torch.cuda.empty_cache()
        print("âœ… Memory cleaned up")


def load_samples_from_json(json_path: str) -> List[Dict[str, Any]]:
    """
    åˆ†æçµæœJSONã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        json_path: åˆ†æçµæœJSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        ã‚µãƒ³ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    samples = []
    results = data.get("results", [])
    
    for result in results:
        question_id = result.get("question_id")
        variations = result.get("variations", [])
        
        for idx, variation in enumerate(variations):
            sample = {
                "question_id": question_id,
                "variation_index": idx,
                "template_type": variation.get("template_type", "(base)"),
                "prompt": variation.get("prompt", ""),
                "response": variation.get("response", "")
            }
            samples.append(sample)
    
    return samples


def save_results_to_json(
    original_json_path: str,
    extraction_results: List[Dict[str, Any]],
    output_json_path: str,
    config: ExtractionConfig
):
    """
    æŠ½å‡ºçµæœã‚’å…ƒã®JSONã«çµ±åˆã—ã¦ä¿å­˜
    
    Args:
        original_json_path: å…ƒã®åˆ†æçµæœJSONãƒ‘ã‚¹
        extraction_results: æŠ½å‡ºçµæœã®ãƒªã‚¹ãƒˆ
        output_json_path: å‡ºåŠ›å…ˆJSONãƒ‘ã‚¹
        config: æŠ½å‡ºè¨­å®š
    """
    # å…ƒã®JSONã‚’èª­ã¿è¾¼ã¿
    with open(original_json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    if "sae_extraction_metadata" not in data:
        data["sae_extraction_metadata"] = {}
    
    data["sae_extraction_metadata"][f"layer_{config.target_layer}"] = {
        "sae_id": config.sae_id,
        "sae_release": config.sae_release,
        "hook_name": config.hook_name,
        "top_k_features": config.top_k_features
    }
    
    # çµæœã‚’çµ±åˆ
    for extract_result in extraction_results:
        question_id = extract_result.get("question_id")
        variation_idx = extract_result.get("variation_index")
        
        if question_id is None or variation_idx is None:
            continue
        
        # è©²å½“ã™ã‚‹variationã‚’æ¢ã—ã¦çµæœã‚’è¿½åŠ 
        for result in data["results"]:
            if result["question_id"] == question_id:
                variations = result["variations"]
                if variation_idx < len(variations):
                    # æ—¢å­˜ã®SAE activationãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«è¿½åŠ 
                    if "sae_activations" not in variations[variation_idx]:
                        variations[variation_idx]["sae_activations"] = {}
                    
                    variations[variation_idx]["sae_activations"][f"layer_{config.target_layer}"] = {
                        "activations": extract_result.get("activations", {}),
                        "answer_start_position": extract_result.get("answer_start_position"),
                        "total_tokens": extract_result.get("total_tokens"),
                        "status": extract_result.get("status")
                    }
                break
    
    # ä¿å­˜
    output_path = Path(output_json_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Results saved to: {output_path}")
