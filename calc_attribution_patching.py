#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Attribution Patching Analysis for Sycophancy Detection
SAEç‰¹å¾´é‡ã®å› æœçš„å¯„ä¸ã‚’åˆ†æã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import sys
import json
import gc
import torch
from typing import Dict, Any, Generator
from pathlib import Path
from tqdm import tqdm
from datetime import datetime
from transformer_lens import HookedTransformer
from sae_lens import SAE

# ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
torch.set_grad_enabled(True)  # å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹åŒ–


def yield_sycophancy_samples(data: Dict[str, Any]) -> Generator[Dict[str, Any], None, None]:
    """
    JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Attribution Patchingç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿
    """
    results = data.get("results", [])

    for result in results:
        variations = result.get("variations", [])
        question_id = result.get("question_id")

        # 1. Baseå›ç­”ã®ç‰¹å®š
        base_variation = None
        base_idx = -1  # Baseã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿æŒ
        for idx, var in enumerate(variations):
            t_type = var.get("template_type")
            if t_type == "base" or t_type == "(base)" or not t_type:
                base_variation = var
                base_idx = idx
                break

        if not base_variation:
            continue

        # 2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆè¿åˆï¼‰å›ç­”ã®ç‰¹å®šã¨ãƒšã‚¢ãƒªãƒ³ã‚°
        for idx, target_variation in enumerate(variations):
            if target_variation is base_variation:
                continue

            if target_variation.get("sycophancy_flag") == 1:
                yield {
                    "question_id": question_id,
                    "variation_index": idx,
                    "base_variation_index": base_idx,  # Baseã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¿”ã™
                    "template_type": target_variation.get("template_type"),
                    "prompt": target_variation.get("prompt"),
                    "target_response": target_variation.get("response"),
                    "base_response": base_variation.get("response")
                }


class AttributionPatchingAnalyzer:
    def __init__(self, model: HookedTransformer, sae: SAE, config: Any):
        self.model = model
        self.sae = sae
        self.config = config
        # hook_name ã‚’ç›´æ¥æŒ‡å®šï¼ˆconfig.py ã‹ã‚‰å–å¾—ã™ã‚‹ã®ãŒç†æƒ³ï¼‰
        # Gemma Scope SAE ã® sae_id ã‹ã‚‰æ¨å®š
        # ä¾‹: "layer_31/width_16k/canonical" â†’ "blocks.31.hook_resid_post"
        sae_id = config.model.sae_id
        layer_num = sae_id.split('/')[0].replace('layer_', '')
        self.hook_name = f"blocks.{layer_num}.hook_resid_post"
        print(f"   ğŸ¯ Using hook: {self.hook_name}")

    def _find_answer_start_position(self, full_tokens: torch.Tensor, prompt_str: str) -> int:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®çµ‚ã‚ã‚Šï¼ˆå›ç­”ã®å§‹ã¾ã‚Šï¼‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã‚’ç‰¹å®šã™ã‚‹
        """
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå˜ä½“ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³é•·ã‚’å–å¾—
        # Note: BOSãƒˆãƒ¼ã‚¯ãƒ³ç­‰ã®æ‰±ã„ã«æ³¨æ„ã€‚Gemmaã¯add_bos_token=TrueãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        prompt_tokens = self.model.to_tokens(prompt_str, prepend_bos=True)
        return prompt_tokens.shape[1] - 1  # 0-indexed ãªã®ã§ -1 (æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®)

    def calculate_atp_for_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        1ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã™ã‚‹Attribution Patchingã‚’å®Ÿè¡Œ
        """
        prompt = sample["prompt"]
        response = sample["target_response"]
        base_response = sample["base_response"]

        # 1. ãƒˆãƒ¼ã‚¯ãƒ³åŒ– (Teacher Forcing Input)
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + å®Ÿéš›ã®å›ç­”
        full_text = prompt + response
        input_tokens = self.model.to_tokens(full_text, prepend_bos=True)

        # å›ç­”é–‹å§‹ä½ç½®ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ï¼‰ã‚’ç‰¹å®š
        # ã“ã“ãŒã€Œæ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå›ç­”ã®1æ–‡å­—ç›®ï¼‰ã€ã‚’äºˆæ¸¬ã™ã‚‹ä½ç½®ã«ãªã‚‹
        target_pos = self._find_answer_start_position(input_tokens, prompt)

        # å…¥åŠ›é•·ãƒã‚§ãƒƒã‚¯ (ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’è¶…ãˆãªã„ã‹)
        if input_tokens.shape[1] > self.model.cfg.n_ctx:
            return {"error": "Sequence too long"}

        # 2. Target Token ã¨ Base Token ã® ID ã‚’å–å¾—
        # Baseå›ç­”ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        base_full_text = prompt + base_response
        base_input_tokens = self.model.to_tokens(base_full_text, prepend_bos=True)
        base_target_pos = self._find_answer_start_position(base_input_tokens, prompt)

        # è¤‡æ•°ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã‚’è©¦ã—ã¦ã€ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãƒšã‚¢ã‚’è¦‹ã¤ã‘ã‚‹
        max_tokens_to_check = 20  # æœ€å¤§20ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§ãƒã‚§ãƒƒã‚¯
        target_token_id = None
        base_token_id = None
        token_offset = 1  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç›´å¾Œã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆ

        for offset in range(1, max_tokens_to_check + 1):
            try:
                candidate_target = input_tokens[0, target_pos + offset].item()
                candidate_base = base_input_tokens[0, base_target_pos + offset].item()

                # ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¦‹ã¤ã‹ã£ãŸã‚‰æ¡ç”¨
                if candidate_target != candidate_base:
                    target_token_id = candidate_target
                    base_token_id = candidate_base
                    token_offset = offset
                    break
            except IndexError:
                # ã©ã¡ã‚‰ã‹ã®å›ç­”ãŒçŸ­ã™ãã‚‹å ´åˆ
                break

        # ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒåŒä¸€ã€ã¾ãŸã¯å–å¾—å¤±æ•—ã®å ´åˆ
        if target_token_id is None or base_token_id is None:
            return {"skipped": "No differing tokens found in first 5 positions"}

        # ä½¿ç”¨ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã‚’æ›´æ–°ï¼ˆLogitå–å¾—ç”¨ï¼‰
        # target_pos ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€å¾Œã®ä½ç½®ãªã®ã§ã€offset-1 ã®ä½ç½®ã®Logitã‚’è¦‹ã‚‹
        logit_pos = target_pos + token_offset - 1

        # 3-0. Baseå›ç­”ã§ã®ç‰¹å¾´é‡å–å¾—ï¼ˆAttribution Patchingã®å·®åˆ†è¨ˆç®—ç”¨ï¼‰
        # å‹¾é…è¨ˆç®—ã¯ä¸è¦ã€å€¤ã®ã¿ä¿å­˜
        self.model.eval()
        base_f_acts = None
        
        with torch.no_grad():
            base_storage = {}
            
            def base_hook(activation, hook):
                """Baseå›ç­”ã®ç‰¹å¾´é‡ã‚’å–å¾—ï¼ˆå‹¾é…ä¸è¦ï¼‰"""
                base_act = activation[:, base_target_pos:base_target_pos+1, :]
                base_storage['acts'] = self.sae.encode(base_act)
                return activation
            
            try:
                _ = self.model.run_with_hooks(
                    base_input_tokens,
                    fwd_hooks=[(self.hook_name, base_hook)]
                )
                base_f_acts = base_storage['acts'].detach().cpu()
            except Exception as e:
                # Baseç‰¹å¾´é‡ã®å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯è­¦å‘Šã®ã¿ï¼ˆå‡¦ç†ã¯ç¶™ç¶šï¼‰
                print(f"âš ï¸ Failed to get base features: {e}")
            finally:
                del base_storage
                torch.cuda.empty_cache()

        # 3. Forward Pass & Metric Calculation
        self.model.zero_grad()

        # ãƒ•ãƒƒã‚¯å†…ã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹ãŸã‚ã®ã‚³ãƒ³ãƒ†ãƒŠ
        feature_acts_storage = {}

        def atp_hook(activation, hook):
            """
            Activationã‚’å–å¾—ã—ã€SAEã‚’é€šã—ã¦å‹¾é…ã‚’æµã™ãƒ•ãƒƒã‚¯
            """
            # activation: [batch, seq, d_model]
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½ç½®ï¼ˆå›ç­”ç›´å‰ï¼‰ã®ã¿ã‚’æŠ½å‡º
            # batch=1 å‰æ
            target_act = activation[:, target_pos:target_pos+1, :]

            # SAE Encode (Feature Activationè¨ˆç®—)
            # SAEã®å…¥åŠ›æ¬¡å…ƒã«åˆã‚ã›ã¦èª¿æ•´
            f_acts = self.sae.encode(target_act)  # [1, 1, n_features]

            # å‹¾é…è¨ˆç®—ã®ãŸã‚ã«ä¿å­˜ (retain_gradé‡è¦)
            f_acts.requires_grad_(True)
            f_acts.retain_grad()
            feature_acts_storage['acts'] = f_acts

            # SAE Decode (Reconstruction)
            x_hat = self.sae.decode(f_acts)

            # Gradient Trick:
            # Forward: å…ƒã®Activation (x) ã‚’ãã®ã¾ã¾æµã™ (Teacher Forcingã®ç²¾åº¦ç¶­æŒ)
            # Backward: Reconstruction (x_hat) ã‚’é€šã—ã¦å‹¾é…ã‚’æµã™ (SAEç‰¹å¾´é‡ã¸ã®Pathã‚’ä½œã‚‹)
            # x_out = x_hat + (x - x_hat).detach()
            # ã“ã‚Œã«ã‚ˆã‚Šã€Metricã®å‹¾é…ã¯ x_hat -> f_acts ã¨ä¼æ’­ã™ã‚‹

            x_out = x_hat + (target_act - x_hat).detach()

            # å…ƒã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«æˆ»ã™
            activation[:, target_pos:target_pos+1, :] = x_out
            return activation

        # ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ
        try:
            logits = self.model.run_with_hooks(
                input_tokens,
                fwd_hooks=[(self.hook_name, atp_hook)]
            )

            # 4. Metric Calculation (Logit Difference)
            # logit_pos ã®ä½ç½®ã§ã®äºˆæ¸¬ã‚’è¦‹ã‚‹ï¼ˆoffsetã«å¿œã˜ãŸä½ç½®ï¼‰
            target_logit = logits[0, logit_pos, target_token_id]
            base_logit = logits[0, logit_pos, base_token_id]
            metric = target_logit - base_logit

            # 5. Backward Pass
            metric.backward()

            # 6. AtP Score Calculation
            # Score = (Targetç‰¹å¾´é‡ - Baseç‰¹å¾´é‡) * Gradient
            # ã“ã‚Œã«ã‚ˆã‚Šã€ŒBaseâ†’Targetã®å¤‰åŒ–ãŒå¼•ãèµ·ã“ã—ãŸåŠ¹æœã€ã‚’æ¸¬å®š
            f_acts = feature_acts_storage['acts']
            f_grad = f_acts.grad

            if f_acts is None or f_grad is None:
                return {"error": "Failed to capture gradients"}

            # Targetç‰¹å¾´é‡ã‚’CPUã«ç§»å‹•
            f_acts_cpu = f_acts.detach().cpu().squeeze()  # [n_features]
            f_grad_cpu = f_grad.detach().cpu().squeeze()  # [n_features]
            
            # Attribution Patching: å·®åˆ† Ã— å‹¾é…
            if base_f_acts is not None:
                base_f_acts_squeezed = base_f_acts.squeeze()  # [n_features]
                delta_f = f_acts_cpu - base_f_acts_squeezed  # Target - Base
                atp_scores = delta_f * f_grad_cpu
            else:
                # Baseã®å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå¾“æ¥ã®æ–¹æ³•ï¼‰
                print("âš ï¸ Using fallback: f_acts * f_grad (no base comparison)")
                atp_scores = f_acts_cpu * f_grad_cpu

            # çµæœã®æŠ½å‡ºï¼ˆTop-K & Non-zeroï¼‰
            # ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚ã€ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã‚‚ã®ã ã‘ã‚’ä¿å­˜
            top_k = 50
            top_indices = torch.topk(atp_scores.abs(), k=top_k).indices

            top_features = []
            for idx in top_indices:
                idx_val = idx.item()
                score = atp_scores[idx_val].item()
                target_act = f_acts_cpu[idx_val].item()
                
                feature_dict = {
                    "id": str(idx_val),
                    "score": score,
                    "target_activation": target_act,
                    "gradient": f_grad_cpu[idx_val].item()
                }
                
                # Baseç‰¹å¾´é‡ãŒã‚ã‚‹å ´åˆã¯å·®åˆ†æƒ…å ±ã‚‚è¿½åŠ 
                if base_f_acts is not None:
                    base_act = base_f_acts.squeeze()[idx_val].item()
                    feature_dict["base_activation"] = base_act
                    feature_dict["activation_delta"] = target_act - base_act
                
                top_features.append(feature_dict)

            return {
                "status": "success",
                "target_token": self.model.to_string(target_token_id),
                "base_token": self.model.to_string(base_token_id),
                "token_position": token_offset,  # ã©ã®ä½ç½®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ã£ãŸã‹è¨˜éŒ²
                "logit_diff": metric.item(),
                "top_features": top_features
            }

        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                torch.cuda.empty_cache()
                return {"error": "OOM"}
            raise e
        finally:
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self.model.zero_grad()
            del feature_acts_storage
            if 'logits' in locals():
                del logits
            if 'f_acts' in locals():
                del f_acts
            if 'f_grad' in locals():
                del f_grad
            if 'base_f_acts' in locals():
                del base_f_acts
            torch.cuda.empty_cache()


def run_attribution_patching_pipeline(
    input_json_path: str = None,
    output_json_path: str = None,
    config_name: str = None,
    layer: int = None
):
    """
    Attribution Patchingåˆ†æã®ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
    Args:
        input_json_path: å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•æ¤œç´¢ï¼‰
        output_json_path: å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        config_name: ä½¿ç”¨ã™ã‚‹configåï¼ˆconfig.pyã‹ã‚‰èª­ã¿è¾¼ã¿ã€layerã¨ä½µç”¨ä¸å¯ï¼‰
        layer: è§£æå¯¾è±¡ã®layerç•ªå·ï¼ˆ9, 20, 31ã‚’ã‚µãƒãƒ¼ãƒˆã€config_nameã‚ˆã‚Šå„ªå…ˆï¼‰
    """
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®å–å¾—
    project_root = Path(__file__).parent.absolute()
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‘ã‚¹è¨­å®š
    if input_json_path is None:
        input_json_path = project_root / "results/labeled_data/combined_feedback_data.json"
    else:
        input_json_path = Path(input_json_path)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
    if not input_json_path.exists():
        # ãƒ†ã‚¹ãƒˆç”¨ã«æœ€æ–°ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        search_dir = project_root / "results/feedback"
        files = list(search_dir.glob("feedback_analysis_*.json"))
        if files:
            input_json_path = sorted(files)[-1]
            print(f"âš ï¸ æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™: {input_json_path}")
        else:
            raise FileNotFoundError(f"Input JSON file not found: {input_json_path}")

    # å‡ºåŠ›ãƒ‘ã‚¹ã®è¨­å®š
    if output_json_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_json_path = project_root / f"results/feedback/atp_results_gemma-2-9b-it_{timestamp}.json"
    else:
        output_json_path = Path(output_json_path)
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    output_json_path.parent.mkdir(parents=True, exist_ok=True)

    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    print(f"ğŸ“‚ Loading data from {input_json_path}...")
    with open(input_json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # 2. Configã®èª­ã¿è¾¼ã¿
    if layer is not None:
        # layerç•ªå·ã‹ã‚‰configã‚’è‡ªå‹•é¸æŠ
        if layer == 9:
            from config import FEEDBACK_GEMMA2_9B_IT_LAYER9_CONFIG
            config = FEEDBACK_GEMMA2_9B_IT_LAYER9_CONFIG
            print(f"   ğŸ“ Using Layer 9 config")
        elif layer == 20:
            from config import FEEDBACK_GEMMA2_9B_IT_LAYER20_CONFIG
            config = FEEDBACK_GEMMA2_9B_IT_LAYER20_CONFIG
            print(f"   ğŸ“ Using Layer 20 config")
        elif layer == 31:
            from config import FEEDBACK_GEMMA2_9B_IT_CONFIG
            config = FEEDBACK_GEMMA2_9B_IT_CONFIG
            print(f"   ğŸ“ Using Layer 31 config")
        else:
            raise ValueError(f"Unsupported layer: {layer}. Supported layers: 9, 20, 31")
    elif config_name is not None:
        # configåã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿
        import config as config_module
        config = getattr(config_module, config_name)
        print(f"   ğŸ“ Using config: {config_name}")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Layer 31
        from config import FEEDBACK_GEMMA2_9B_IT_CONFIG
        config = FEEDBACK_GEMMA2_9B_IT_CONFIG
        print(f"   ğŸ“ Using default Layer 31 config")

    # 3. ãƒ¢ãƒ‡ãƒ«ã¨SAEã®æº–å‚™
    print("ğŸ”„ Loading Model & SAE...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"   Using device: {device}")

    model = HookedTransformer.from_pretrained_no_processing(
        config.model.name,
        device=device,
        dtype=torch.bfloat16
    )

    # SAEãƒ­ãƒ¼ãƒ‰
    sae = SAE.from_pretrained(
        release=config.model.sae_release,
        sae_id=config.model.sae_id,
        device=device
    )

    analyzer = AttributionPatchingAnalyzer(model, sae, config)

    # 4. ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
    samples = list(yield_sycophancy_samples(data))
    print(f"ğŸš€ Starting ATP analysis for {len(samples)} samples...")
    print(f"ğŸ’¾ Results will be saved to: {output_json_path}")

    for i, sample in enumerate(tqdm(samples)):
        res = analyzer.calculate_atp_for_sample(sample)

        # å…ƒã®JSONã«çµæœã‚’çµ±åˆ
        question_id = sample["question_id"]
        variation_idx = sample["variation_index"]
        base_variation_idx = sample["base_variation_index"]

        # ãƒ‡ãƒãƒƒã‚°: çµæœã®å†…å®¹ã‚’è¡¨ç¤ºï¼ˆæœ€åˆã®æ•°ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ï¼‰
        if i < 3:
            print(f"\nğŸ” Sample {i} result: {res}")

        # è©²å½“ã™ã‚‹variationã‚’æ¢ã—ã¦ atp_analysis ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
        for result in data["results"]:
            if result["question_id"] == question_id:
                variations = result["variations"]
                if variation_idx < len(variations):
                    if res.get("status") == "success":
                        # atp_analysis ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ä¿å­˜
                        variations[variation_idx]["atp_analysis"] = {
                            "top_features": res["top_features"],
                            "target_token": res["target_token"],
                            "base_token": res["base_token"],
                            "token_position": res["token_position"],
                            "logit_diff": res["logit_diff"]
                        }
                        
                        # â˜… sae_activations ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã‚‚ä¿å­˜ï¼ˆå¾Œç¶šã‚¹ã‚¯ãƒªãƒ—ãƒˆç”¨ï¼‰ â˜…
                        activation_key = "prompt_last_token"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚­ãƒ¼å
                        
                        # Targetæ´»æ€§å€¤ã®è¾æ›¸ã‚’ä½œæˆ {feature_id: activation}
                        target_acts_dict = {
                            f["id"]: f["target_activation"] 
                            for f in res["top_features"]
                        }
                        
                        # sae_activations ãŒãªã‘ã‚Œã°ä½œæˆ
                        if "sae_activations" not in variations[variation_idx]:
                            variations[variation_idx]["sae_activations"] = {}
                        
                        variations[variation_idx]["sae_activations"][activation_key] = target_acts_dict
                        
                        # Base variation ã«ã‚‚æ´»æ€§å€¤ã‚’ä¿å­˜ï¼ˆLog Ratioè¨ˆç®—ç”¨ï¼‰
                        if base_variation_idx >= 0 and base_variation_idx < len(variations):
                            base_var = variations[base_variation_idx]
                            
                            # Baseæ´»æ€§å€¤ã®è¾æ›¸ã‚’ä½œæˆ
                            base_acts_dict = {
                                f["id"]: f["base_activation"]
                                for f in res["top_features"]
                                if "base_activation" in f
                            }
                            
                            if "sae_activations" not in base_var:
                                base_var["sae_activations"] = {}
                            
                            # æ—¢å­˜ã®å€¤ãŒã‚ã‚Œã°ãƒãƒ¼ã‚¸ï¼ˆè¤‡æ•°ã®Targetã‹ã‚‰å‚ç…§ã•ã‚Œã‚‹å¯èƒ½æ€§ï¼‰
                            if activation_key not in base_var["sae_activations"]:
                                base_var["sae_activations"][activation_key] = {}
                            
                            base_var["sae_activations"][activation_key].update(base_acts_dict)
                        
                    else:
                        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€è©³ç´°æƒ…å ±ã‚‚ä¿å­˜
                        variations[variation_idx]["atp_analysis"] = {
                            "error": res.get("error") or res.get("skipped") or "unknown",
                            "details": res
                        }
                        # æœ€åˆã®ã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤º
                        if i < 10:
                            print(f"âš ï¸ Sample {i} (Q{question_id}, Var{variation_idx}): {res}")
                break

        # å®šæœŸçš„ã«ä¿å­˜
        if (i + 1) % 10 == 0:
            with open(output_json_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            gc.collect()
            torch.cuda.empty_cache()

    # æœ€çµ‚ä¿å­˜
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

    print(f"âœ… Analysis completed. Saved to {output_json_path}")


def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰ã®å®Ÿè¡Œç”¨ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Attribution Patching Analysis for Sycophancy Detection"
    )
    parser.add_argument(
        "--input",
        type=str,
        default=None,
        help="Input JSON file path (default: results/labeled_data/combined_feedback_data.json)"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="Output JSON file path (default: auto-generated with timestamp)"
    )
    parser.add_argument(
        "--layer",
        type=int,
        default=None,
        choices=[9, 20, 31],
        help="Layer number to analyze (9, 20, or 31). Overrides --config if specified."
    )
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="Config name to use from config.py (ignored if --layer is specified)"
    )
    
    args = parser.parse_args()
    
    try:
        run_attribution_patching_pipeline(
            input_json_path=args.input,
            output_json_path=args.output,
            config_name=args.config,
            layer=args.layer
        )
    except Exception as e:
        print(f"âŒ Error during execution: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
