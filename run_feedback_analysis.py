"""
ğŸ§  Feedbackè¿åˆæ€§åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€Feedbackãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦LLMã®è¿åˆæ€§ï¼ˆSycophancyï¼‰åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

ç‰¹å¾´:
- Gemma-2-9B-itå¯¾å¿œ: å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã®è©³ç´°åˆ†æ
- SAEå†…éƒ¨çŠ¶æ…‹æŠ½å‡º: Layer 9/20/31ã®SAEæ´»æ€§åŒ–ã‚’è©³ç´°ã«è¨˜éŒ²
- æœ€é©ãªåˆ†æä½ç½®: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå¿œç­”ç”Ÿæˆç›´å‰ï¼‰ã®å†…éƒ¨çŠ¶æ…‹ã‚’å–å¾—
- ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ¯”è¼ƒ: 5ç¨®é¡ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã®å¿œç­”ã‚’æ¯”è¼ƒ

å®Ÿè¡Œæ–¹æ³•:
    python run_feedback_analysis.py
"""

import os
import sys
import warnings
import torch
import json
import time
import datetime
from copy import deepcopy
from pathlib import Path
import pandas as pd
import numpy as np

# ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š
warnings.filterwarnings('ignore')
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from feedback_analyzer import FeedbackAnalyzer
    from config import (
        FEEDBACK_GEMMA2_9B_IT_CONFIG,
        FEEDBACK_GEMMA2_9B_IT_LAYER20_CONFIG,
        FEEDBACK_GEMMA2_9B_IT_LAYER9_CONFIG,
        FeedbackConfig
    )
    print("âœ… ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†")
except ImportError as e:
    print(f"âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ’¡ feedback_analyzer.py ã¨ config.py ãŒåŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)


def setup_experiment_config(
    base_config,
    save_all_tokens=False,
    max_new_tokens=512,
    temperature=0.7,
    verbose=True,
    show_prompts=True,
    show_responses=True,
    response_tokens_to_capture=8
):
    """
    å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
    
    Args:
        base_config: ãƒ™ãƒ¼ã‚¹è¨­å®šï¼ˆFEEDBACK_GEMMA2_9B_IT_CONFIGç­‰ï¼‰
        save_all_tokens: å…¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿å­˜ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Falseï¼‰
        max_new_tokens: ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆ0.0-1.0ï¼‰
        verbose: è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤º
        show_prompts: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¡¨ç¤º
        show_responses: å¿œç­”ã‚’è¡¨ç¤º
        response_tokens_to_capture: å¿œç­”ã®æœ€åˆã®ä½•ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã™ã‚‹ã‹
    
    Returns:
        è¨­å®šã•ã‚ŒãŸExperimentConfig
    """
    config = deepcopy(base_config)
    
    # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    config.generation.max_new_tokens = max_new_tokens
    config.generation.temperature = temperature
    
    # ãƒ‡ãƒãƒƒã‚°è¨­å®š
    config.debug.verbose = verbose
    config.debug.show_prompts = show_prompts
    config.debug.show_responses = show_responses
    
    # Feedbackå°‚ç”¨è¨­å®š
    if not hasattr(config, 'feedback'):
        config.feedback = FeedbackConfig()
    config.feedback.save_all_tokens = save_all_tokens
    config.feedback.response_tokens_to_capture = response_tokens_to_capture
    
    return config


def print_config_summary(config, start_index=None, end_index=None):
    """è¨­å®šã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    print("ğŸ¯ å®Ÿé¨“è¨­å®š:")
    print(f"   ğŸ“± ãƒ¢ãƒ‡ãƒ«: {config.model.name}")
    if start_index is not None and end_index is not None:
        print(f"   ğŸ± Start Question ID: {start_index}, End Question ID: {end_index}")
    print(f"   ğŸ“Š ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ•°: 5ç¨®é¡/å•é¡Œ")
    print(f"   ğŸ’¾ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆ†æä½ç½®: {'å…¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒˆãƒ¼ã‚¯ãƒ³' if config.feedback.save_all_tokens else 'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€çµ‚ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå¿œç­”ç”Ÿæˆç›´å‰ï¼‰'}")
    print(f"   ğŸ’¬ å¿œç­”åˆ†æ: æœ€åˆã®{config.feedback.response_tokens_to_capture}ãƒˆãƒ¼ã‚¯ãƒ³")
    print(f"   ğŸ¯ å¯¾è±¡ãƒ¬ã‚¤ãƒ¤ãƒ¼: {config.feedback.target_layer}")
    print(f"   ğŸŒ¡ï¸  æ¸©åº¦: {config.generation.temperature}")
    print(f"   ğŸ”¢ æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³: {config.generation.max_new_tokens}")
    print(f"   ğŸ” è©³ç´°ãƒ­ã‚°: {config.debug.verbose}")
    print(f"   ğŸ” SAE: {config.model.sae_release}/{config.model.sae_id}")


def check_dataset_path(config):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯"""
    ds_path = config.data.dataset_path
    if not os.path.exists(ds_path):
        print(f"\nâš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {ds_path}")
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã‚’è©¦è¡Œ
        default_file = os.path.join('eval_dataset', 'feedback.jsonl')
        if os.path.exists(default_file):
            config.data.dataset_path = default_file
            print(f"   âœ… è‡ªå‹•è£œæ­£: dataset_path ã‚’ {default_file} ã«å¤‰æ›´ã—ã¾ã—ãŸ")
        else:
            print("   âŒ eval_dataset/feedback.jsonl ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            sys.exit(1)


def run_analysis(config, start_index=None, end_index=None):
    """
    Feedbackè¿åˆæ€§åˆ†æã‚’å®Ÿè¡Œ
    
    Args:
        config: ExperimentConfig
        start_index: é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-basedï¼‰
        end_index: çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-basedï¼‰
    
    Returns:
        åˆ†æçµæœã®ãƒªã‚¹ãƒˆ
    """
    print("ğŸš€ Feedbackè¿åˆæ€§åˆ†æã‚’é–‹å§‹...")
    print("=" * 60)
    
    # å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    experiment_metadata = {
        'experiment_start': datetime.datetime.now().isoformat(),
        'config_params': {
            'model_name': config.model.name,
            'save_all_tokens': config.feedback.save_all_tokens,
            'response_tokens_to_capture': config.feedback.response_tokens_to_capture,
            'target_layer': config.feedback.target_layer,
            'temperature': config.generation.temperature,
            'max_new_tokens': config.generation.max_new_tokens,
            'start_index': start_index,
            'end_index': end_index,
        }
    }
    
    print(f"ğŸ“‹ å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿:")
    print(f"   â° é–‹å§‹æ™‚åˆ»: {experiment_metadata['experiment_start']}")
    print(f"   ğŸ“Š è¨­å®š: {experiment_metadata['config_params']}")
    
    # åˆ†æå™¨ã®åˆæœŸåŒ–
    print(f"\nğŸ”§ åˆ†æå™¨ã‚’åˆæœŸåŒ–ä¸­...")
    analyzer = FeedbackAnalyzer(config)
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã®è¡¨ç¤º
    if torch.cuda.is_available():
        memory_used = torch.cuda.memory_allocated(0) / 1024**3
        memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ’¾ åˆæœŸGPU Memory: {memory_used:.1f}GB / {memory_total:.1f}GB")
        experiment_metadata['initial_memory_gb'] = round(memory_used, 2)
    
    # å®Œå…¨åˆ†æã®å®Ÿè¡Œ
    print(f"\nğŸ”„ å®Œå…¨åˆ†æã‚’å®Ÿè¡Œä¸­...")
    print(f"   ğŸ“‹ ã“ã®å‡¦ç†ã«ã¯æ•°åˆ†ã€œæ•°ååˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...")
    
    # å®Ÿè¡Œæ™‚é–“æ¸¬å®šé–‹å§‹
    start_time = time.time()
    
    # ãƒ¡ã‚¤ãƒ³åˆ†æã®å®Ÿè¡Œ
    analyzer.run_complete_analysis(start_index=start_index, end_index=end_index)
    
    # å®Ÿè¡Œæ™‚é–“æ¸¬å®šçµ‚äº†
    end_time = time.time()
    execution_time = end_time - start_time
    experiment_metadata['execution_time_seconds'] = round(execution_time, 2)
    experiment_metadata['experiment_end'] = datetime.datetime.now().isoformat()
    
    print(f"\n" + "=" * 60)
    print("âœ… åˆ†æå®Œäº†ï¼")
    print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {execution_time:.1f}ç§’ ({execution_time/60:.1f}åˆ†)")
    
    # çµæœã®ç°¡æ˜“è¡¨ç¤º
    if hasattr(analyzer, 'results') and analyzer.results:
        num_questions = len(analyzer.results)
        total_variations = sum(len(r.variations) for r in analyzer.results)
        
        # å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«çµæœã‚’è¿½åŠ 
        experiment_metadata['results'] = {
            'num_questions': num_questions,
            'total_variations': total_variations,
        }
        
        print(f"\nğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼:")
        print(f"   ğŸ“ åˆ†æã—ãŸå•é¡Œæ•°: {num_questions}")
        print(f"   ğŸ“ˆ ç·ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æ•°: {total_variations}")
        print(f"   ğŸ’¾ ãƒˆãƒ¼ã‚¯ãƒ³ä¿å­˜ãƒ¢ãƒ¼ãƒ‰: {'å…¨ãƒˆãƒ¼ã‚¯ãƒ³' if config.feedback.save_all_tokens else 'æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿'}")
        
        # æœ€åˆã®çµæœã®æ¦‚è¦ã‚’è¡¨ç¤º
        if num_questions > 0:
            first_result = analyzer.results[0]
            print(f"\nğŸ“„ æœ€åˆã®è³ªå•ã®æ¦‚è¦:")
            print(f"   Dataset: {first_result.dataset}")
            print(f"   ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ•°: {len(first_result.variations)}")
            for var in first_result.variations[:3]:  # æœ€åˆã®3ã¤ã ã‘è¡¨ç¤º
                print(f"   - {var.prompt_info.prompt_template_type or '(base)'}: {len(var.response_text)} æ–‡å­—")
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã®æœ€çµ‚ç¢ºèª
    if torch.cuda.is_available():
        memory_used = torch.cuda.memory_allocated(0) / 1024**3
        print(f"\nğŸ’¾ æœ€çµ‚GPU Memory: {memory_used:.1f}GB / {memory_total:.1f}GB")
        experiment_metadata['final_memory_gb'] = round(memory_used, 2)
    
    # å®Ÿé¨“ãƒ­ã‚°ã®ä¿å­˜
    experiment_log_file = f"experiment_log_{experiment_metadata['experiment_start'][:19].replace(':', '-')}.json"
    with open(experiment_log_file, 'w', encoding='utf-8') as f:
        json.dump(experiment_metadata, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“‹ å®Ÿé¨“ãƒ­ã‚°ä¿å­˜: {experiment_log_file}")
    
    return analyzer.results


def analyze_results(results, config):
    """
    åˆ†æçµæœã‚’è©³ç´°ã«ç¢ºèª
    
    Args:
        results: åˆ†æçµæœã®ãƒªã‚¹ãƒˆ
        config: ExperimentConfig
    """
    print("\nğŸ“ˆ åˆ†æçµæœã®è©³ç´°ã‚’è¡¨ç¤º...")
    print("=" * 60)
    
    # åŸºæœ¬çµ±è¨ˆ
    num_questions = len(results)
    total_variations = sum(len(r.variations) for r in results)
    
    print(f"ğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
    print(f"   ğŸ“ åˆ†æã—ãŸå•é¡Œæ•°: {num_questions}")
    print(f"   ğŸ“ˆ ç·ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æ•°: {total_variations}")
    print(f"   ğŸ’¾ å¹³å‡ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æ•°/å•é¡Œ: {total_variations/num_questions:.1f}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ã®çµ±è¨ˆ
    dataset_counts = {}
    for result in results:
        dataset = result.dataset
        dataset_counts[dataset] = dataset_counts.get(dataset, 0) + 1
    
    print(f"\nğŸ“š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ã®å†…è¨³:")
    for dataset, count in dataset_counts.items():
        print(f"   - {dataset}: {count} å•é¡Œ")
    
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚¿ã‚¤ãƒ—åˆ¥ã®çµ±è¨ˆ
    template_stats = {}
    for result in results:
        for variation in result.variations:
            template_type = variation.prompt_info.prompt_template_type or "(base)"
            if template_type not in template_stats:
                template_stats[template_type] = {
                    'count': 0,
                    'avg_response_length': 0,
                    'total_length': 0,
                    'avg_active_features': 0,
                    'total_features': 0
                }
            
            stats = template_stats[template_type]
            stats['count'] += 1
            stats['total_length'] += len(variation.response_text)
            stats['total_features'] += len(variation.top_k_features)
    
    # å¹³å‡ã‚’è¨ˆç®—
    for template_type, stats in template_stats.items():
        stats['avg_response_length'] = stats['total_length'] / stats['count']
        stats['avg_active_features'] = stats['total_features'] / stats['count']
    
    print(f"\nğŸ“ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚¿ã‚¤ãƒ—åˆ¥ã®çµ±è¨ˆ:")
    for template_type, stats in template_stats.items():
        print(f"   {template_type}:")
        print(f"      ã‚µãƒ³ãƒ—ãƒ«æ•°: {stats['count']}")
        print(f"      å¹³å‡å¿œç­”é•·: {stats['avg_response_length']:.0f} æ–‡å­—")
        print(f"      å¹³å‡æ´»æ€§åŒ–ç‰¹å¾´æ•°: {stats['avg_active_features']:.1f}")
    
    # å…·ä½“ä¾‹ã®è¡¨ç¤ºï¼ˆæœ€åˆã®2å•ï¼‰
    print(f"\nğŸ“ å…·ä½“ä¾‹ï¼ˆæœ€åˆã®2å•ï¼‰:")
    print("-" * 60)
    
    for i, result in enumerate(results[:2]):
        print(f"\nå•é¡Œ {i+1} (ID: {result.question_id}):")
        print(f"  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {result.dataset}")
        print(f"  ãƒ™ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆ: {result.base_text[:100]}...")
        print(f"  ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ•°: {len(result.variations)}")
        
        for j, variation in enumerate(result.variations):
            template_type = variation.prompt_info.prompt_template_type or "(base)"
            print(f"\n  ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ {j+1}: {template_type}")
            print(f"    å¿œç­”: {variation.response_text[:150]}...")
            print(f"    å¿œç­”é•·: {len(variation.response_text)} æ–‡å­—")
            print(f"    Top-3 SAEç‰¹å¾´:")
            for feat_id, feat_val in variation.top_k_features[:3]:
                print(f"      Feature {feat_id}: {feat_val:.4f}")
            print(f"    ç”Ÿæˆæ™‚é–“: {variation.metadata.get('generation_time_ms', 0):.0f} ms")
    
    # DataFrameã«å¤‰æ›ã—ã¦ä¿å­˜
    print(f"\nğŸ’¾ çµæœã‚’DataFrameã«å¤‰æ›ä¸­...")
    
    rows = []
    for result in results:
        for variation in result.variations:
            row = {
                'question_id': result.question_id,
                'dataset': result.dataset,
                'template_type': variation.prompt_info.prompt_template_type or "(base)",
                'response_length': len(variation.response_text),
                'num_active_features': len(variation.top_k_features),
                'top_feature_id': variation.top_k_features[0][0] if variation.top_k_features else None,
                'top_feature_value': variation.top_k_features[0][1] if variation.top_k_features else None,
                'generation_time_ms': variation.metadata.get('generation_time_ms', 0)
            }
            rows.append(row)
    
    df_results = pd.DataFrame(rows)
    
    print(f"âœ… DataFrameä½œæˆå®Œäº† ({len(df_results)} è¡Œ)")
    print(f"\nDataFrame ã‚µãƒ³ãƒ—ãƒ«:")
    print(df_results.head(10))
    
    # CSVã«ä¿å­˜
    csv_path = f"results/feedback/feedback_analysis_summary.csv"
    os.makedirs("results/feedback", exist_ok=True)
    df_results.to_csv(csv_path, index=False)
    print(f"\nğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}")
    
    print(f"\n" + "=" * 60)
    print("âœ… çµæœç¢ºèªå®Œäº†ï¼")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸ§  Feedbackè¿åˆæ€§åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    
    # GPUç¢ºèª
    if torch.cuda.is_available():
        try:
            print(f"GPUæ¤œå‡º: {torch.cuda.get_device_name(0)}")
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"GPU Memory: {memory_total:.1f}GB")
        except Exception:
            print("GPUæ¤œå‡º: åˆ©ç”¨å¯èƒ½ (ãƒ‡ãƒã‚¤ã‚¹åã®å–å¾—ã«å¤±æ•—)")
    else:
        print("âŒ GPUåˆ©ç”¨ä¸å¯ (CPUãƒ¢ãƒ¼ãƒ‰)")
    
    # === å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š ===
    # ã“ã“ã‚’å¤‰æ›´ã—ã¦å®Ÿé¨“æ¡ä»¶ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ã¦ãã ã•ã„
    
    # ãƒ™ãƒ¼ã‚¹è¨­å®šã‚’é¸æŠï¼ˆä»¥ä¸‹ã‹ã‚‰1ã¤ã‚’é¸æŠï¼‰
    # base_config = FEEDBACK_GEMMA2_9B_IT_CONFIG  # Layer 31
    base_config = FEEDBACK_GEMMA2_9B_IT_LAYER20_CONFIG  # Layer 20
    # base_config = FEEDBACK_GEMMA2_9B_IT_LAYER9_CONFIG  # Layer 9
    
    # åˆ†æç¯„å›²ã®è¨­å®š
    start = 0  # é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-basedï¼‰
    end = 10    # çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-basedï¼‰
    
    # å®Ÿé¨“è¨­å®šã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
    config = setup_experiment_config(
        base_config=base_config,
        save_all_tokens=False,  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ï¼ˆæ¨å¥¨ï¼‰
        max_new_tokens=512,     # ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        temperature=0.7,        # ç”Ÿæˆæ¸©åº¦
        verbose=True,           # è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤º
        show_prompts=True,      # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¡¨ç¤º
        show_responses=True,    # å¿œç­”ã‚’è¡¨ç¤º
        response_tokens_to_capture=8  # å¿œç­”ã®æœ€åˆã®8ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
    )
    
    # è¨­å®šã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    print_config_summary(config, start_index=start, end_index=end)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    check_dataset_path(config)
    
    print(f"\nâœ… å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šå®Œäº†")
    
    # === åˆ†æã®å®Ÿè¡Œ ===
    try:
        results = run_analysis(config, start_index=start, end_index=end)
        
        # === çµæœã®åˆ†æ ===
        if results:
            analyze_results(results, config)
        
        print(f"\nğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        sys.exit(1)


if __name__ == "__main__":
    main()
